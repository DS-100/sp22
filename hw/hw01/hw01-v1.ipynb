{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw01-v1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1: Math Review and Plotting\n",
    "## Due Date: Thursday Jan 27, 11:59 PM\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Assignment\n",
    "\n",
    "The purpose of this assignment is for you to combine Python, math, and the ideas in Data 8 to draw some interesting conclusions. The methods and results will help build the foundation of Data 100.\n",
    "\n",
    "## Score Breakdown\n",
    "Question | Points\n",
    "--- | --- \n",
    "1a | 1\n",
    "1b | 2\n",
    "2a | 1 \n",
    "2b | 1\n",
    "2c | 2\n",
    "2d | 2\n",
    "2e | 1\n",
    "3a | 2\n",
    "3b | 2\n",
    "3c | 1\n",
    "3d | 2\n",
    "3e | 2\n",
    "4a | 1\n",
    "4b | 1 \n",
    "4c | 1\n",
    "4d | 1\n",
    "5a | 1\n",
    "5b | 1\n",
    "5c | 3\n",
    "6a | 2\n",
    "6b(i) | 2\n",
    "6b(ii) | 2\n",
    "6c | 2\n",
    "Total | 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells *below* your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "### Initialize your environment\n",
    "\n",
    "This cell should run without error if you're using the course Jupyter Hub or you have [set up your personal computer correctly](http://www.ds100.org/sp20/setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: Jupyter Shortcuts ###\n",
    "\n",
    "Here are some useful Jupyter notebook keyboard shortcuts.  To learn more keyboard shortcuts, go to **Help -> Keyboard Shortcuts** in the menu above. \n",
    "\n",
    "Here are a few we like:\n",
    "1. `ctrl`+`return` : *Evaluate the current cell*\n",
    "1. `shift`+`return`: *Evaluate the current cell and move to the next*\n",
    "1. `esc` : *command mode* (may need to press before using any of the commands below)\n",
    "1. `a` : *create a cell above*\n",
    "1. `b` : *create a cell below*\n",
    "1. `dd` : *delete a cell*\n",
    "1. `m` : *convert a cell to markdown*\n",
    "1. `y` : *convert a cell to code*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: NumPy ###\n",
    "\n",
    "You should be able to understand the code in the following cells. If not, review the following:\n",
    "\n",
    "* [The Data 8 Textbook Chapter on NumPy](https://www.inferentialthinking.com/chapters/05/1/Arrays)\n",
    "* [DS100 NumPy Review](http://ds100.org/fa17/assets/notebooks/numpy/Numpy_Review.html)\n",
    "* [Condensed NumPy Review](http://cs231n.github.io/python-numpy-tutorial/#numpy)\n",
    "* [The Official NumPy Tutorial](https://numpy.org/doc/stable/user/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter pro-tip**: Pull up the docs for any function in Jupyter by running a cell with\n",
    "the function name and a `?` at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can close the window at the bottom by pressing `esc` several times or clicking on the x at the right hand side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Jupyter pro-tip**: Pull up the docs for any function in Jupyter by typing the function\n",
    "name, then `<Shift><Tab>` on your keyboard. This is super convenient when you forget the order\n",
    "of the arguments to a function. You can press `<Tab>` multiple times to expand the docs and reveal additional information.\n",
    "\n",
    "Try it on the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: LaTeX ###\n",
    "You should use LaTeX to format math in your answers. If you aren't familiar with LaTeX, not to worry. It's not hard to use in a Jupyter notebook. Just place your math in between dollar signs within Markdown cells:\n",
    "\n",
    "`$ f(x) = 2x $` becomes $ f(x) = 2x $.\n",
    "\n",
    "If you have a longer equation, use double dollar signs to place it on a line by itself:\n",
    "\n",
    "`$$ \\sum_{i=0}^n i^2 $$` becomes:\n",
    "\n",
    "$$ \\sum_{i=0}^n i^2$$\n",
    "\n",
    "\n",
    "You can align multiple lines using the `&` anchor, `\\\\` newline, in an `align` block as follows:\n",
    "\n",
    "```\n",
    "\\begin{align}\n",
    "f(x) &= (x - 1)^2 \\\\\n",
    "&= x^2 - 2x + 1\n",
    "\\end{align}\n",
    "```\n",
    "becomes\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= (x - 1)^2 \\\\\n",
    "&= x^2 - 2x + 1\n",
    "\\end{align}\n",
    "\n",
    "* [This PDF](latex_tips.pdf) has some handy LaTeX.\n",
    "* [For more about basic LaTeX formatting, you can read this article.](https://www.sharelatex.com/learn/Mathematical_expressions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: Sums ###\n",
    "\n",
    "Here's a recap of some basic algebra written in sigma notation. The facts are all just applications of the ordinary associative and distributive properties of addition and multiplication, written compactly and without the possibly ambiguous \"...\". But if you are ever unsure of whether you're working correctly with a sum, you can always try writing $\\sum_{i=1}^n a_i$ as $a_1 + a_2 + \\cdots + a_n$ and see if that helps.\n",
    "\n",
    "You can use any reasonable notation for the index over which you are summing, just as in Python you can use any reasonable name in `for name in list`. Thus $\\sum_{i=1}^n a_i = \\sum_{k=1}^n a_k$.\n",
    "\n",
    "- $\\sum_{i=1}^n (a_i + b_i) = \\sum_{i=1}^n a_i + \\sum_{i=1}^n b_i$\n",
    "- $\\sum_{i=1}^n d = nd$\n",
    "- $\\sum_{i=1}^n (ca_i + d) = c\\sum_{i=1}^n a_i + nd$\n",
    "\n",
    "These properties may be useful in the Least Squares Predictor question. To see the LaTeX we used, double-click this cell. Evaluate the cell to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 1: Calculus ##\n",
    "\n",
    "In this question we will review some fundamental properties of the sigmoid function, which will be discussed when we talk more about logistic regression in the latter half of the class. The sigmoid function is defined to be\n",
    "$$\\sigma(x) = \n",
    "\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "### Question 1a ####\n",
    "Show that $\\sigma(-x) = 1 - \\sigma(x)$.\n",
    "\n",
    "**Note, again: In this class, you must always put your answer in the cell that immediately follows the question. DO NOT create any cells between this one and the one that says** _Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1b ###\n",
    "Show that the derivative of the sigmoid function can be written as:\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "[This PDF](latex_tips.pdf) has some handy LaTeX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Probabilities and Proportions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Much of data analysis involves interpreting proportions â€“ lots and lots of related proportions. So let's recall the basics. It might help to start by reviewing [the main rules](https://www.inferentialthinking.com/chapters/09/5/Finding_Probabilities.html) from Data 8, with particular attention to what's being multiplied in the multiplication rule.\n",
    "\n",
    "\n",
    "### Question 2a ###\n",
    "The Pew Research Foundation publishes the results of numerous surveys, one of which is about the [trust that Americans have](https://www.pewresearch.org/fact-tank/2019/03/22/public-confidence-in-scientists-has-remained-stable-for-decades/) in groups such as the military, scientists, and elected officials to act in the public interest. A table in the article summarizes the results.\n",
    "\n",
    "Pick one of the options (i) and (ii) to answer the question below; if you pick (i), fill in the blank with the percent. Then, explain your choice.\n",
    "\n",
    "The percent of surveyed U.S. adults who had a great deal of confidence in both scientists and religious leaders\n",
    "\n",
    "(i) is equal to ______________________.\n",
    "\n",
    "(ii) cannot be found with the information in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 2b ###\n",
    "In a famous (or infamous) survey, members of the Harvard medical school were asked to consider a scenario in which \"a test to detect a disease whose prevalence is 1/1,000 has a false positive rate of 5 percent\". The terminology, the specific question asked in the survey, and the answer, are discussed in detail in a Stat 88 textbook [section](http://stat88.org/textbook/notebooks/Chapter_02/04_Use_and_Interpretation.html#Harvard-Medical-School-Survey) that you are strongly encouraged to read. As Stat 88 is a Data 8 connector course, the section is another look at the same ideas as in the corresponding [Data 8 textbook section](https://www.inferentialthinking.com/chapters/18/2/Making_Decisions.html).\n",
    "\n",
    "The corresponding tree diagram is copied below for your reference.\n",
    "\n",
    "<img src=\"tree_disease_harvard.png\">\n",
    "\n",
    "The survey did not provide the true positive rate. The respondents and Stat 88 were allowed to assume that the true positive rate is 1, but we will not do so here. **Let the true positive rate be some unknown proportion $p$.**\n",
    "\n",
    "Suppose a person is picked at random from the population. Let $N$ be the event that the person doesn't have the disease and let $T_N$ be the event that the person's test result is negative. \n",
    "\n",
    "Fill in Blanks 1 and 2 with options chosen from (1)-(9).\n",
    "\n",
    "The proportion $P(N \\mid T_N)$ is the number of people who $\\underline{~~~~~~1~~~~~~}$ relative to the total number of people who $\\underline{~~~~~~2~~~~~~}$.\n",
    "\n",
    "(1) are in the population\n",
    "\n",
    "(2) have the disease\n",
    "\n",
    "(3) don't have the disease\n",
    "\n",
    "(4) test positive\n",
    "\n",
    "(5) test negative\n",
    "\n",
    "(6) have the disease and test positive\n",
    "\n",
    "(7) have the disease and test negative\n",
    "\n",
    "(8) don't have the disease and test positive\n",
    "\n",
    "(9) don't have the disease and test negative\n",
    "\n",
    "Assign the variable `q4bi` to your answer to the first blank and `q4bii` to your answer to the second blank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4bi = ...\n",
    "q4bii = ...\n",
    "q4bi, q4bii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c ###\n",
    "(This is a continuation of the previous part.) Define a function `no_disease_given_negative` that takes $p$ as its argument and returns $P(N \\mid T_N)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def no_disease_given_negative(p):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2d ###\n",
    "(This part is a continuation of the previous two.) Pick all of the options (i)-(iv) that are true for all values of $p$. Explain by algebraic or probabilistic reasoning; you are welcome to use your function `no_disease_given_negative` to try a few cases numerically. Your explanation should include the reasons why you *didn't* choose some options.\n",
    "\n",
    "$P(N \\mid T_N)$ is\n",
    "\n",
    "(i) equal to $0.95$.\n",
    "\n",
    "(ii) equal to $0.999 \\times 0.95$.\n",
    "\n",
    "(iii) greater than $0.999 \\times 0.95$.\n",
    "\n",
    "(iv) greater than $0.95$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for experimenting if you wish, but your answer should be written in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2e ###\n",
    "Suzuki is one of most commonly owned makes of cars in our county (Alameda). A car heading from Berkeley to San Francisco is pulled over on the freeway for speeding. Suppose I tell you that the car is either a Suzuki or a Lamborghini, and you have to guess which of the two is more likely. \n",
    "\n",
    "What would you guess, and why? Make some reasonable assumptions and explain them (data scientists often have to do this), justify your answer, and say how it's connected to the previous parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3: Distributions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing distributions, both categorical and numerical, helps us understand variability. In Data 8 you visualized numerical distributions by drawing [histograms](https://www.inferentialthinking.com/chapters/07/2/Visualizing_Numerical_Distributions.html#A-Histogram), which look like bar charts but represent proportions by the *areas* of the bars instead of the heights or lengths. In this exercise you will use the `hist` function in `matplotlib` instead of the corresponding `Table` method to draw histograms.\n",
    "\n",
    "To start off, suppose we want to plot the probability distribution of the number of spots on a single roll of a die. That should be a flat histogram since the chance of each of the values 1 through 6 is 1/6. Here is a first attempt at drawing the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = range(1, 7)\n",
    "plt.hist(faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This default plot is not helpful. We have to choose some arguments to get a visualization that we can interpret. \n",
    "\n",
    "Note that the second printed line shows the left ends of the default bins, as well as the right end of the last bin. The first line shows the counts in the bins. If you don't want the printed lines you can add a semi-colon at the end of the call to `plt.hist`, but we'll keep the lines for now.\n",
    "\n",
    "Let's redraw the histogram with bins of unit length centered at the possible values. By the end of the exercise you'll see a reason for centering. Notice that the argument for specifying bins is the same as the one for the `Table` method `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_bins = np.arange(0.5, 6.6)\n",
    "plt.hist(faces, bins = unit_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to see the edges of the bars! Let's specify the edge color `ec` to be white. [Here](https://matplotlib.org/3.1.0/gallery/color/named_colors.html) are all the colors you could use, but do try to drag yourself away from the poetic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(faces, bins = unit_bins, ec='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better, but look at the vertical axis. It is not drawn to the [density scale](https://www.inferentialthinking.com/chapters/07/2/Visualizing_Numerical_Distributions.html#The-Histogram:-General-Principles-and-Calculation) defined in Data 8. We want a histogram of a probability distribution, so the total area should be 1. We just have to ask for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(faces, bins = unit_bins, ec='white', density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the probability histogram of the number of spots on one roll of a die. The proportion is $1/6$ in each of the bins.\n",
    "\n",
    "**Note**: You may notice that running the above cells also displayed the return value of the last function call of each cell. This was intentional on our part to show you how `plt.hist()` ([documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)) returned different values per plot.\n",
    "\n",
    "**Note 2**: Going forward, you can use a semicolon `;` on the last line to suppress additional display, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(faces, bins = unit_bins, ec='white', density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3a ###\n",
    "\n",
    "Define a function `integer_distribution` that takes an array of integers and draws the histogram of the distribution using unit bins centered at the integers and white edges for the bars. The histogram should be drawn to the density scale. The left-most bar should be centered at the smallest integer in the array, and the right-most bar at the largest.\n",
    "\n",
    "Your function does not have to check that the input is an array consisting only of integers. The display does not need to include the printed proportions and bins.\n",
    "\n",
    "If you have trouble defining the function, go back and carefully read all the lines of code that resulted in the probability histogram of the number of spots on one roll of a die. Pay special attention to the bins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def integer_distribution(x):\n",
    "    ...\n",
    "integer_distribution(faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 3b ###\n",
    "(Note: You can complete this part with just prerequisite knowledge for Data 100. That being said, Lecture 2 provides additional historical context and definitions for probability sample, sampling bias, and chance error).\n",
    "\n",
    "One way to use probability samples is to quantify sampling bias and chance error. Put briefly, if we assume that a sample distribution was selected at random from a known population, then we can quantify how likely that sample is to have arisen due to random chance (**chance error**). If the difference in sample and population distributions is too great, then we suspect that the given sample has **bias** in how it was selected from the population.\n",
    "\n",
    "Let's see this process in a *post*-analysis of *pre*-election polling of the 1936 U.S. Presidential Election. Through the U.S. electoral college process (we'll ignore it in this question, but read more [here](https://en.wikipedia.org/wiki/United_States_Electoral_College)), Franklin D. Roosevelt won the election by an overwhelming margin. The popular vote results were approximately 61% Roosevelt (Democrat, incumbent), 37% Alf Landon (Republican), and 2% other candidates. For this problem, this is our **population distribution**. \n",
    "\n",
    "You can use `np.random.multinomial` to simulate drawing at random with replacement from a categorical distribution. The arguments are the sample size `n` and an array `pvals` of the proportions in all the categories. The function simulates `n` independent random draws from the distribution and returns the observed counts in all the categories. Read the documentation to see how this is described formally; we will use the formal terminology and notation in future assignments after we have discussed them in class.\n",
    "\n",
    "You will see that the function also takes a third argument `size`, which for our purposes will be an integer that specifies the number of times to run the entire simulation. All the runs are independent of each other. \n",
    "\n",
    "Write one line of code that uses `np.random.multinomial` to run 10 independent simulations of drawing 100 times at random with replacement from a population in which 61% of the people vote for Roosevelt, 37% for Landon, and 2% for other candidatdes. The output should be an array containing the counts in the **Roosevelt** category in the 10 simulations. It will help to recall how to slice `NumPy` arrays. Assign your answer to the variable `sample`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = ...\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3c ###\n",
    "\n",
    "Replace the \"...\" in the code cell below with a Python expression so that the output of the cell is an empirical histogram of 500,000 simulated counts of voters for Roosevelt in 100 draws made at random with replacement from the voting population.  \n",
    "\n",
    "After you have drawn the histogram, you might want to take a moment to recall the conclusion reached by the *Literary Digest*, a magazine that---while having successfully predicted the outcome of many previous presidential elections---failed to correctly predict the winner of the 1936 presidential election. In their survey of 10 million individuals, they predicted the popular vote as just 43% for Roosevelt and 57% for Landon. Based on our simulation, there was most definitely sampling bias in the *Digest*'s sampling process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulated_counts = ...\n",
    "integer_distribution(simulated_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3d ###\n",
    "As you know, the count of Roosevelt voters in a sample of 100 people drawn at random from the eligible population is expected to be 61. Just by looking at the histogram in Part **c**, and **no other calculation**, pick the correct option and **explain your choice**. You might want to refer to the [Data 8 textbook](https://www.inferentialthinking.com/chapters/14/3/SD_and_the_Normal_Curve.html) again.\n",
    "\n",
    "The SD of the distribution of the number of Roosevelt voters in a random sample of 100 people drawn from the eligible population is closest to\n",
    "\n",
    "(i) 1.9\n",
    "\n",
    "(ii) 4.9\n",
    "\n",
    "(iii) 10.9\n",
    "\n",
    "(iv) 15.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3e ###\n",
    "The *normal curve with mean $\\mu$ and SD $\\sigma$* is defined by\n",
    "\n",
    "$$\n",
    "f(x) ~ = ~ \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\big{(}\\frac{x-\\mu}{\\sigma}\\big{)}^2}, ~~~ -\\infty < x < \\infty\n",
    "$$\n",
    "\n",
    "Redraw your histogram from Part **c** and overlay the normal curve with $\\mu = 61$ and $\\sigma$ equal to the choice you made in Part **d**. You just have to call `plt.plot` after `integer_distribution`. Use `np.e` for $e$. For the curve, use 2 as the line width, and any color that is easy to see over the blue histogram. It's fine to just let Python use its default color.\n",
    "\n",
    "Now you can see why centering the histogram bars over the integers was a good idea. The normal curve peaks at 26, which is the center of the corresponding bar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = ...\n",
    "sigma = ...\n",
    "x = np.linspace(40, 80, 200)\n",
    "f_x = ...\n",
    "integer_distribution(simulated_counts)\n",
    "#plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4: Linear Algebra ##\n",
    "A common representation of data uses matrices and vectors, so it is helpful to familiarize ourselves with linear algebra notation, as well as some simple operations.\n",
    "\n",
    "Define a vector $\\vec{v}$ to be a column vector. Then, the following properties hold:\n",
    "\n",
    "* $c\\vec{v}$ with $c$ some constant $c \\in \\mathbb{R}$, is equal to a new vector where every element in $c\\vec{v}$ is equal to the corresponding element in $\\vec{v}$ multiplied by $c$. For example, $2 \\begin{bmatrix}\n",
    "     1 \\\\\n",
    "     2 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "     2 \\\\\n",
    "     4 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "* $\\vec{v}_1 + \\vec{v}_2$ is equal to a new vector with elements equal to the elementwise addition of $\\vec{v}_1$ and $\\vec{v}_2$. For example, $\\begin{bmatrix}\n",
    "     1 \\\\\n",
    "     2 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "     -3 \\\\\n",
    "     4 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    -2 \\\\\n",
    "     6 \\\\\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "The above properties form our definition for a **linear combination** of vectors. $\\vec{v}_3$ is a linear combination of $\\vec{v}_1$ and $\\vec{v}_2$ if $\\vec{v}_3 = a\\vec{v}_1 + b\\vec{v}_2$, where $a$ and $b$ are some constants.\n",
    "\n",
    "Oftentimes, we stack column vectors to form a matrix. Define the **rank** of a matrix $A$ to be equal to the maximal number of linearly independent columns in $A$. A set of columns is **linearly independent** if no column can be written as a linear combination of any other column(s) within the set.\n",
    "\n",
    "For example, let $A$ be a matrix with 4 columns. If three of these columns are linearly independent, but the fourth can be written as a linear combination of the other three, then $\\text{rank}(A) = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**For each part below**, you will be presented with a set of vectors, and a matrix consisting of those vectors stacked in columns.\n",
    "1. State the rank of the matrix, and clearly state whether or not the matrix is full rank.\n",
    "1. If the matrix is *not* full rank, state a linear relationship among the vectorsâ€”for example: $\\vec{v}_1 = 2\\vec{v}_2$.\n",
    "\n",
    "\n",
    "### Question 4a ###\n",
    "\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix}\n",
    "     1 \\\\\n",
    "     0 \\\\\n",
    "\\end{bmatrix}\n",
    ", \n",
    "\\vec{v}_2 = \\begin{bmatrix}\n",
    "     1 \\\\\n",
    "     1 \\\\\n",
    "\\end{bmatrix}\n",
    ", A = \\begin{bmatrix}\n",
    "    \\vert & \\vert \\\\\n",
    "    \\vec{v}_1 & \\vec{v}_2   \\\\\n",
    "    \\vert & \\vert\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "### Question 4b ###\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix}\n",
    "     3 \\\\\n",
    "     -4 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\vec{v}_2 = \\begin{bmatrix}\n",
    "     0 \\\\\n",
    "     0 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "B = \\begin{bmatrix}\n",
    "    \\vert & \\vert \\\\\n",
    "    \\vec{v}_1 & \\vec{v}_2   \\\\\n",
    "    \\vert & \\vert\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "### Question 4c ###\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix}\n",
    "     0 \\\\\n",
    "     1 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\vec{v}_2 = \\begin{bmatrix}\n",
    "     5 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\vec{v}_3 = \\begin{bmatrix}\n",
    "     10 \\\\\n",
    "     10 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "C = \\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert \\\\\n",
    "    \\vec{v}_1 & \\vec{v}_2 & \\vec{v}_3    \\\\\n",
    "    \\vert & \\vert & \\vert\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "### Question 4d ###\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix}\n",
    "     0 \\\\\n",
    "     2 \\\\\n",
    "     3 \\\\\n",
    "\\end{bmatrix}\n",
    ", \n",
    "\\vec{v}_2 = \\begin{bmatrix}\n",
    "     -2 \\\\\n",
    "    -2 \\\\\n",
    "     5 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\vec{v}_3 = \\begin{bmatrix}\n",
    "     2 \\\\\n",
    "     4 \\\\\n",
    "     -2 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "D = \\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert \\\\\n",
    "    \\vec{v}_1 & \\vec{v}_2 & \\vec{v}_3    \\\\\n",
    "    \\vert & \\vert & \\vert\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5: A Least Squares Predictor ##\n",
    "Let the list of numbers $(x_1, x_2, \\ldots, x_n)$ be data. You can think of each index $i$ as the label of a household, and the entry $x_i$ as the annual income of Household $i$. Define the **mean** or **average** $\\mu$ of the list to be\n",
    "$$\\mu ~ = ~ \\frac{1}{n}\\sum_{i=1}^n x_i.$$\n",
    "\n",
    "\n",
    "### Question 5a ###\n",
    "The $i$th *deviation from average* is the difference $x_i - \\mu$. In Data 8 you saw in numerical examples that the [sum of all these deviations is 0](https://www.inferentialthinking.com/chapters/14/2/Variability.html#The-Rough-Size-of-Deviations-from-Average). Now prove that fact. That is, show that $\\sum_{i=1}^n (x_i - \\mu) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5b ###\n",
    "[Recall](https://www.inferentialthinking.com/chapters/14/2/Variability.html#The-Rough-Size-of-Deviations-from-Average) that the **variance** of a list is defined as the *mean squared deviation from average*, and that the [**standard deviation**](https://www.inferentialthinking.com/chapters/14/2/Variability.html#Standard-Deviation) (SD) of the list is the square root of the variance. The SD is in the same units as the data and measures the rough size of the deviations from average.\n",
    "\n",
    "Denote the variance of the list by $\\sigma^2$. Write a math expression for $\\sigma^2$ in terms of the data ($x_{1} \\dots x_{n}$) and $\\mu$. We recommend building your expression by reading the definition of variance from right to left. That is, start by writing the notation for \"average\", then \"deviation from average\", and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Mean Squared Error\n",
    "Suppose you have to predict the value of $x_i$ for some $i$, but you don't get to see $i$ and you certainly don't get to see $x_i$. You decide that whatever $x_i$ is, you're just going to use some number $c$ as your *predictor*.\n",
    "\n",
    "The *error* in your prediction is $x_i - c$. Thus the **mean squared error** (MSE) of your predictor $c$ over the entire list of $n$ data points can be written as:\n",
    "\n",
    "$$MSE(c) = \\frac{1}{n}\\sum_{i=1}^n (x_i - c)^2.$$\n",
    "\n",
    "You may already see some similarities to your definition of variance from above! You then start to wonderâ€”if you picked your favorite number $c = \\mu$ as the predictor, would it be \"better\" than other choices $c \\neq \\mu$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "### Question 5c\n",
    "One common approach to defining a \"best\" predictor is as predictor that *minimizes* the MSE on the data $(x_1, \\dots, x_n)$.\n",
    "\n",
    "In this course, we commonly use calculus to find the predictor $c$ as follows:\n",
    "1. Define $MSE$ to be a function of $c$, i.e., $MSE(c)$ as above. Assume that the data points $x_1, x_2, ..., x_n$ are fixed, and that $c$ is the only variable. \n",
    "2. Determine the value of $c$ that minimizes $MSE(c)$.\n",
    "3. Justify that this is indeed a minimum, not a maximum.\n",
    "\n",
    "Step 1 is done for you in the problem statement; follow steps 2 and 3 to show that $\\mu$ is the value of $c$ that minimizes $MSE(c)$. You must do both steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Your proof above shows that $\\mu$ is the **least squares** *constant* predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 6: A More Familiar Least Squares Predictor\n",
    "In Data 8 you found (numerically) the [least squares *linear* predictor](https://www.inferentialthinking.com/chapters/15/3/Method_of_Least_Squares.html) of a variable $y$ based on a related variable $x$. In this course, we will prove your findings using a generalization of your calculation in the previous question.\n",
    "\n",
    "When we get to this proof later in this course, you will need to be comfortable with vector operations. For now, you will get familiar with this notation by rewriting your least squares findings from Data 8 (and the previous question) using vector notation. **This question won't require you to write LaTeX**, so just focus on the mathematical notation we're presenting.\n",
    "\n",
    "### The Dot Product\n",
    "(1) We start by defining the **dot product** of two *real* vectors\n",
    "$x = \\begin{bmatrix}\n",
    "     x_1 \\\\\n",
    "     x_2 \\\\\n",
    "     \\dots \\\\\n",
    "     x_n\n",
    "     \\end{bmatrix}$\n",
    "and \n",
    "$y = \\begin{bmatrix}\n",
    "     y_1 \\\\\n",
    "     y_2 \\\\\n",
    "     \\dots \\\\\n",
    "     y_n\n",
    "\\end{bmatrix}$ as follows:\n",
    "\n",
    "$$x^T y = \\sum_{i=1}^n x_i y_i $$\n",
    "* Given the above definition, the dot product is (1) a **scalar**, not another vector; and (2) only defined for two vectors of the same length.\n",
    "* **Note**: In this course we often opt for $x$ instead of $\\vec{x}$ to simplify notation; $x$ as a vector is inferred from its use in the dot product. Then $x_i$ is the $i$-th element of the vector $x$.\n",
    "* *Detail*: In this course, we prefer the notation $x^Ty$ to illustrate a dot product, defined as matrix multiplication of $x^T$ and $y$. In the literature you may also see $x \\cdot y$, but we avoid this notation since the dot ($\\cdot$) notation is occasionally used for scalar values.\n",
    "* *Detail*: The dot product is a special case of an inner product, where $x, y \\in \\mathbb{R}^n$.\n",
    "\n",
    "(2) We introduce a special vector, $\\mathbb{1}$, to write the [**mean**](https://inferentialthinking.com/chapters/14/1/Properties_of_the_Mean.html) $\\bar{x}$ of data $(x_1, x_2, \\dots, x_n)$ as a dot product:\n",
    "\\begin{align}\n",
    "\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\sum_{i=1}^n 1x_i \\\\\n",
    "        &= \\frac{1}{n}(x^T\\mathbb{1}).\n",
    "\\end{align}\n",
    "* The data $(x_1, \\dots, x_n)$ have been defined as an $n$-dimensional column vector $x$, where $x = \\begin{bmatrix}\n",
    "     x_1 \\\\\n",
    "     x_2 \\\\\n",
    "     \\dots \\\\\n",
    "     x_n\n",
    "     \\end{bmatrix}$.\n",
    "* The special vector $\\mathbb{1}$ is a **vector of ones**, whose length is defined by the vector operation in which it is used. So with $n$-dimensional column vector $x$, the dot product $x^T\\mathbb{1}$ implies that $\\mathbb{1}$ is an $n$-dimensional column vector where every element is $1$.\n",
    "* Because dot products produce scalars, the multiplication of two scalars $\\frac{1}{n}$ and $x^T\\mathbb{1}$ produces another scalar, $\\bar{x}$.\n",
    "* **Note**: We use bar notation for the mean ($\\bar{x}$ instead of $\\mu$) in this problem to differentiate $\\bar{x}$ from $\\bar{y}$, the latter of which is the mean of data $(y_1, \\dots, y_n)$.\n",
    "\n",
    "(3) We can further use this definition of $\\bar{x}$ to additionally write the [**variance**](https://www.inferentialthinking.com/chapters/14/2/Variability.html#The-Rough-Size-of-Deviations-from-Average) $\\sigma_x^2$ of the data $(x_1, \\dots, x_n)$ as a dot product. Verify for yourself that the below operation defines $\\sigma_x^2$ as a scalar:\n",
    "\\begin{align}\n",
    "\\sigma_x^2 &= \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\\\\n",
    "        &= \\frac{1}{n}(x - \\bar{x})^T(x - \\bar{x}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6a ###\n",
    "\n",
    "To verify your understanding of the dot product as defined above, suppose you are working with $n$ datapoints $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$.\n",
    "* Define the $x$ data as $(x_1, \\dots, x_n)$ and the $y$ data as $(y_1, \\dots, y_n)$, and define $x$ and $y$ as two $n$-dimensional column vectors, where the $i$-th elements of $x$ and $y$ are $x_i$ and $y_i$, respectively.\n",
    "* Define $\\bar{x}$ and $\\bar{y}$ as the means of the $x$ data and $y$ data, respectively.\n",
    "* Define $\\sigma_x^2$ and $\\sigma_y^2$ as the variances of the $x$ data and $y$ data, respectively. Therefore $\\sigma_x = \\sqrt{\\sigma_x^2}$ and $\\sigma_y = \\sqrt{\\sigma_y^2}$ are the [**standard deviations**](https://inferentialthinking.com/chapters/14/2/Variability.html?highlight=standard%20deviation#standard-deviation) of the $x$ data and $y$ data, respectively.\n",
    "\n",
    "**Suppose** $n = 32$. What is the **dimension** of each of the following expressions? \n",
    "\n",
    "Expression (i). Note there are two ways it is written in the literature.\n",
    "$$\\dfrac{1}{\\sigma_x} (x - \\bar{x}) = \\dfrac{x - \\bar{x}}{\\sigma_x} $$\n",
    "\n",
    "Expression (ii).\n",
    "$$\\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$\n",
    "\n",
    "Assign the variables `q6a_i` and `q6a_ii` to an integer representing the dimension of the above expressions (i) and (ii), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q6a_i = ...\n",
    "q6a_ii = ...\n",
    "\n",
    "# do not modify these lines\n",
    "print(f\"Q6a(i) is {q6a_i}-dimensional\")\n",
    "print(f\"Q6a(ii) is {q6a_ii}-dimensional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products in NumPy\n",
    "\n",
    "Next, we'll use NumPy's matrix multiplication operators to compute expressions for the **regression line**, which you learned in Data 8 was the unique line that minimizes the mean squared error of estimation among all straight lines. At this time, it may be helpful to review the [Data 8 section](https://inferentialthinking.com/chapters/15/2/Regression_Line.html#the-equation-of-the-regression-line).\n",
    "\n",
    "Before we continue, let's contextualize our computation by loading in a [dataset](https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html) you saw in Data 8: the relation between weight lifted and shot put distance among surveyed female collegiate athletes. We've plotted the point using matplotlib's [scatter](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) function, which you will see in more detail in two weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot the data.\n",
    "weight_lifted = np.array([ 37.5,  51.5,  61.3,  61.3,  63.6,  66.1,  70. ,  92.7,  90.5,\n",
    "        90.5,  94.8,  97. ,  97. ,  97. , 102. , 102. , 103.6, 100.4,\n",
    "       108.4, 114. , 115.3, 114.9, 114.7, 123.6, 125.8, 119.1, 118.9,\n",
    "       141.1])\n",
    "shot_put_distance = np.array([ 6.4, 10.2, 12.4, 13. , 13.2, 13. , 12.7, 13.9, 15.5, 15.8, 15.8,\n",
    "       16.8, 17.1, 17.8, 14.8, 15.5, 16.1, 16.2, 17.9, 15.9, 15.8, 16.7,\n",
    "       17.6, 16.8, 17. , 18.2, 19.2, 18.6])\n",
    "\n",
    "plt.scatter(weight_lifted, shot_put_distance)\n",
    "plt.xlabel(\"Weight Lifted\")\n",
    "plt.ylabel(\"Shot Put Distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty linear! Let's try to fit a regression line to this data.\n",
    "\n",
    "Define the vectors $x$ as the weight lifted data vector and $y$ as the shot put distance data vector, respectively, of the college athletes. Then the regression line uses the weight lifted $x$ to predict $\\hat{y}$, which is the **linear estimate** of the actual value shot put distance $y$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\hat{a} + \\hat{b}{x}\\text{, where} \\\\\n",
    "\\hat{a} &= \\bar{y} - \\hat{b}\\bar{x} \\\\\n",
    "\\hat{b} &= r \\dfrac{\\sigma_y}{\\sigma_x}\n",
    "\\end{align}\n",
    "\n",
    "* $\\bar{x}, \\bar{y}$ and $\\sigma_x, \\sigma_y$ are the means and standard deviations, respectively of the data $x$ and $y$, respectively. Here, $r$ is the correlation coefficient as defined in Data 8! \n",
    "* **Note**: We use the hat $\\hat{}$ notation to indicate values we are *estimating*: $\\hat{y}$, the predicted shot put distance, as well as $\\hat{a}$ and $\\hat{b}$, the respective estimated intercept and slope parameters we are using to model the \"best\" linear predictor of $y$ from $x$. We'll dive into this later in the course.\n",
    "* **Note**: Remember how we dropped the $\\vec{}$ vector notation? These linear regression equations therefore represent both the scalar case (predict a single value $\\hat{y}$ from a single $x$) *and* the vector case (predict a vector $\\hat{y}$ element-wise from a vector $x$). How convenient!!\n",
    "\n",
    "In this part, instead of using NumPy's built-in statistical functions like `np.mean()` and `np.std()`, you are going to use NumPy's matrix operations to create the components of the regression line from first principles.\n",
    "\n",
    "The `@` operator multiplies NumPy matrices or arrays together ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul)). We can use this operator to write functions to compute statistics on data, using the expressions that we defined in part (a). Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell.\n",
    "def dot_mean(arr):\n",
    "    n = len(arr)\n",
    "    all_ones = np.ones(n)   # creates n-dimensional vector of ones\n",
    "    return (arr.T @ all_ones)/n\n",
    "\n",
    "def dot_var(arr):\n",
    "    n = len(arr)\n",
    "    mean = dot_mean(arr)\n",
    "    zero_mean_arr = arr - mean\n",
    "    return (zero_mean_arr.T @ zero_mean_arr)/n\n",
    "\n",
    "def dot_std(arr):\n",
    "    return np.sqrt(dot_var(arr))\n",
    "\n",
    "print(\"np.mean(weight_lifted)  =\", np.mean(weight_lifted),\n",
    "      \"\\tdot_mean(weight_lifted) =\", dot_mean(weight_lifted))\n",
    "print(\"np.var(weight_lifted)   =\", np.std(weight_lifted),\n",
    "      \"\\tdot_var(weight_lifted   =\", dot_var(weight_lifted))\n",
    "print(\"np.std(weight_lifted)   =\", np.std(weight_lifted),\n",
    "      \"\\tdot_std(weight_lifted   =\", dot_std(weight_lifted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, you will write code to define the expressions you explored in part (a) of this question.\n",
    "\n",
    "### Question 6b (i) ###\n",
    "\n",
    "Use the NumPy `@` operator to compute expression (i) from part (a). For convenience, we've rewritten the expression below.\n",
    "Note that this expression is also referred to as $x$ in **standard units** ([Data 8 textbook section](https://inferentialthinking.com/chapters/14/2/Variability.html#standard-units)).\n",
    "\n",
    "$$\\dfrac{x - \\bar{x}}{\\sigma_x} $$\n",
    "\n",
    "\n",
    "Write the body of the function `dot_su` which takes in a 1-D NumPy array `arr` and returns `arr` in standard units.\n",
    "* **Do not use `np.mean(), np.std(), np.var(), np.sum()` nor any Python loops.**\n",
    "* You should only use a *subset* of `@, /, +, -, len()`, the `dot_mean(), dot_var(), and dot_std()` functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot_su(arr):\n",
    "    ...\n",
    "\n",
    "# do not edit below this line\n",
    "q6bi_su = dot_su(weight_lifted)\n",
    "q6bi_su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6bi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6b (ii) ###\n",
    "\n",
    "Next use the NumPy `@` operator to compute the correlation coefficient $r$, which is expression (ii) from part (a). For convenience, we've rewritten the expression below.\n",
    "\n",
    "$$r = \\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$\n",
    "\n",
    "Write the body of the function `dot_corr_coeff` which takes in two 1-D NumPy arrays `x` and `y` and returns the correlation coefficient of `x` and `y`.\n",
    "* As before, **Do not use `np.mean(), np.std(), np.var(), np.sum()` nor any Python loops.**\n",
    "* As before, you should only use a *subset* of `@, /, +, -, len()`, the `dot_mean(), dot_var(), and dot_std()` functions defined above.\n",
    "* You may also use the `dot_su()` function that you defined in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot_corr_coeff(x, y):\n",
    "    ...\n",
    "\n",
    "# do not edit below this line\n",
    "q6bii_r = dot_corr_coeff(weight_lifted, shot_put_distance)\n",
    "q6bii_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6bii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6c ###\n",
    "\n",
    "We're ready to put everything together! Finally, use the `dot_`-prefixed functions in this question to compute the regression line. For convenience, we've rewritten the expressions below. $\\hat{y}$ is the linear estimate of the value $y$ based on $x$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\hat{a} + \\hat{b}{x}\\text{, where} \\\\\n",
    "\\hat{a} &= \\bar{y} - \\hat{b}\\bar{x} \\\\\n",
    "\\hat{b} &= r \\dfrac{\\sigma_y}{\\sigma_x}\n",
    "\\end{align}\n",
    "\n",
    "Define the functions `compute_a_hat` and `compute_b_hat` which return the intercept and slope, respectively, of the regression line defind above for a linear estimator of `y` using `x`. Verify how the functions are used to plot the linear regression line (implemented for you).\n",
    "* As before, **Do not use `np.mean(), np.std(), np.var(), np.sum()`, or any for loops.**\n",
    "* You may use a *subset* of `@, /, +, -, len(), dot_mean(), dot_var(), dot_std(), dot_su(), dot_corr_coeff()`.\n",
    "* **Hint:** You may want to define a_hat in terms of b_hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_a_hat(x, y):\n",
    "    ...\n",
    "\n",
    "def compute_b_hat(x, y):\n",
    "    ...\n",
    "\n",
    "# do not edit below this line\n",
    "a_hat = compute_a_hat(weight_lifted, shot_put_distance)\n",
    "b_hat = compute_b_hat(weight_lifted, shot_put_distance)\n",
    "shot_put_hats = a_hat + b_hat * weight_lifted\n",
    "plt.scatter(weight_lifted, shot_put_distance) # the actual data\n",
    "plt.plot(weight_lifted, shot_put_hats, color='g', alpha=0.5) # the prediction line, transparent green\n",
    "plt.xlabel(\"Weight Lifted\")\n",
    "plt.ylabel(\"Shot Put Distance\")\n",
    "display(compute_a_hat(weight_lifted, shot_put_distance))\n",
    "display(compute_b_hat(weight_lifted, shot_put_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "otter": {
   "tests": {
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 <= q4bi <= 9\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= q4bii <= 9\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert all(sample <= 100)\n>>> all(sample >= 0)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(sample) == 10\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(no_disease_given_negative(1), 1)\n>>> np.isclose(no_disease_given_negative(0), 0.998947423819799)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> float(q6a_i).is_integer() and q6a_i >= 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> float(q6a_ii).is_integer() and q6a_ii >= 0\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6bi": {
     "name": "q6bi",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q6bi_su) == len(weight_lifted)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.mean(q6bi_su), 0)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.var(q6bi_su), 1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6bii": {
     "name": "q6bii",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 <= q6bii_r and q6bii_r <= 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(compute_a_hat(weight_lifted, shot_put_distance)) in [float, np.float64]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(compute_b_hat(weight_lifted, shot_put_distance)) in [float, np.float64]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
