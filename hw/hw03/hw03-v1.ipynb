{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw03-v1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Text Analysis Using Twitter\n",
    "\n",
    "**Due Date: Thursday,  February 10, 11:59 PM PT**\n",
    "\n",
    "Welcome to the third homework assignment of Data 100! In this assignment, we will be exploring tweets from several high profile Twitter users.  \n",
    "\n",
    "In this assignment you will gain practice with:\n",
    "* Conducting Data Cleaning and EDA on a text-based dataset.\n",
    "* Manipulating data in pandas with the datetime and string accessors.\n",
    "* Writing regular expressions and using pandas regex methods.\n",
    "* Performing sentiment analysis on social media using VADER.\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from ds100_utils import *\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "def horiz_concat_df(dict_of_df, head=None):\n",
    "    \"\"\"\n",
    "    Horizontally concatenante multiple DataFrames for easier visualization. \n",
    "    Each DataFrame must have the same columns.\n",
    "    \"\"\"\n",
    "    df = pd.concat([df.reset_index(drop=True) for df in dict_of_df.values()], axis=1, keys=dict_of_df.keys())\n",
    "    if head is None:\n",
    "        return df\n",
    "    return df.head(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "1a | 1\n",
    "1b | 1\n",
    "1c | 3\n",
    "1d | 1\n",
    "2a | 2\n",
    "2b | 2\n",
    "2c | 2\n",
    "2d | 2\n",
    "2e | 2\n",
    "2f | 1\n",
    "3a | 1\n",
    "3b | 1\n",
    "3c | 1\n",
    "4a | 1\n",
    "4b | 1\n",
    "4ci | 1\n",
    "4cii | 1\n",
    "4d | 1\n",
    "4e | 2\n",
    "4f | 2\n",
    "4g | 2\n",
    "5a | 2\n",
    "5b | 2\n",
    "**Total** | **35**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Importing the Data\n",
    "\n",
    "\n",
    "The data for this assignment was obtained using the [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api).  To ensure that everyone has the same data and to eliminate the need for every student to apply for a Twitter developer account, we have collected a sample of tweets from several high-profile public figures.  The data is stored in the folder `data`.  Run the following cell to list the contents of the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "from os import listdir\n",
    "for f in listdir(\"data\"):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Question 1a\n",
    "\n",
    "Let's examine the contents of one of these files.  Using the [`open` function](https://docs.python.org/3/library/functions.html#open) and [`read` operation](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) on a python file object, read the first 1000 **characters** in `data/BernieSanders_recent_tweets.txt` and store your result in the variable `q1a`.  Then display the result so you can read it.\n",
    "\n",
    "**Caution:** Viewing the contents of large files in a Jupyter notebook could crash your browser.  Be careful not to print the entire contents of the file.\n",
    "\n",
    "**Hint:** You might want to try to use `with`:\n",
    "\n",
    "```python\n",
    "with open(\"filename\", \"r\") as f:\n",
    "    f.read(2)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Question 1b\n",
    "\n",
    "What format is the data in? Answer this question by entering the letter corresponding to the right format in the variable `q1b` below.\n",
    "\n",
    "A. CSV<br/>\n",
    "B. HTML<br/>\n",
    "C. JavaScript Object Notation (JSON)<br/>\n",
    "D. Excel XML\n",
    "\n",
    "Answer in the following cell. Your answer should be a string, either `\"A\"`, `\"B\"`, `\"C\"`, or `\"D\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1b = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Pandas has built-in readers for many different file formats including the file format used here to store tweets.  To learn more about these, check out the documentation for [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), [`pd.read_html`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html), [`pd.read_json`](https://pandas.pydata.org/docs/reference/api/pandas.io.json.read_json.html#pandas.io.json.read_json), and [`pd.read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).  \n",
    "\n",
    "1. Use one of these functions to populate the `tweets` dictionary with the tweets for: `AOC`, `Cristiano`, and `elonmusk`. The keys of `tweets` should be the handles of the users, which we have provided in the cell below, and the values should be the DataFrames.\n",
    "2. Set the index of each DataFrame to correspond to the `id` of each tweet.  \n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You might want to first try loading one of the DataFrames before trying to complete the entire question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = {\n",
    "    \"AOC\": ...,\n",
    "    \"Cristiano\": ...,\n",
    "    \"elonmusk\": ...,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything correctly, the following cells will show you the first 5 tweets for Elon Musk (and a lot of information about those tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "tweets[\"elonmusk\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "## Question 1d\n",
    "There are many ways we could choose to read tweets. Why might someone be interested in doing data analysis on tweets? Name a kind of person or institution which might be interested in this kind of analysis. Then, give two reasons why a data analysis of tweets might be interesting or useful for them. Answer in 2-3 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "<br/><br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 2:  Source Analysis\n",
    "\n",
    "\n",
    "In some cases, the Twitter feed of a public figure may be partially managed by a public relations firm. In these cases, the device used to post the tweet may help reveal whether it was the individual (e.g., from an iPhone) or a public relations firm (e.g., TweetDeck).  The tweets we have collected contain the source information but it is formatted strangely :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "tweets[\"Cristiano\"][[\"source\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we will use a regular expression to convert this messy HTML snippet into something more readable.  For example: `<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>` should be `Twitter for iPhone`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Question 2a\n",
    "\n",
    "We will first use the Python `re` library to cleanup the above test string.  In the cell below, write a regular expression that will match the **HTML tag** and assign it to the variable `q2a_pattern`. We then use the `re.sub` function to substitute anything that matches the pattern with an empty string `\"\"`.\n",
    "\n",
    "An HTML tag is defined as a `<` character followed by zero or more non-`>` characters, followed by a `>` character. That is `<a>` and `</a>` are both considered _separate_ HTML tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q2a_pattern = r\"...\"\n",
    "test_str = '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
    "re.sub(q2a_pattern, \"\", test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Question 2b\n",
    "\n",
    "Rather than writing a regular expression to detect and remove the HTML tags we could instead write a regular expression to **capture** the device name between the angle brackets.  Here we will use [**capturing groups**](https://docs.python.org/3/howto/regex.html#grouping) by placing parenthesis around the part of the regular expression we want to return.  For example, to capture the `21` in the string `08/21/83` we could use the pattern `r\"08/(..)/83\"`.  \n",
    "\n",
    "\n",
    "**Hint:** The output of the following cell should be `['Twitter for iPhone']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q2b_pattern = r\"...\"\n",
    "test_str = '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
    "re.findall(q2b_pattern, test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2c\n",
    "\n",
    "Using either of the two regular expressions you just created and [`Series.str.replace`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html) or [`Series.str.extract`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html), add a new column called `\"device\"` to **all** of the DataFrames in `tweets` containing just the text describing the device (without the HTML tags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2d\n",
    "\n",
    "To examine the most frequently used devices by each individual, implement the `most_freq` function that takes in a `Series` and returns a new `Series` containing the `k` most commonly occuring entries in the first series, where the values are the counts of the entries and the indices are the entries themselves.\n",
    "\n",
    "For example: \n",
    "```python\n",
    "most_freq(pd.Series([\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"]), k=2)\n",
    "```\n",
    "would return:\n",
    "```\n",
    "A    3\n",
    "B    2\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hint** Consider using `value_counts`, `sort_values`, `head`, and/or `nlargest` (for the last one, read the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.nlargest.html?highlight=nlargest)).\n",
    " Think of what might be the most efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_freq(series, k = 5):\n",
    "    ...\n",
    "\n",
    "most_freq(tweets[\"Cristiano\"]['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the following two cells to compute a table and plot describing the top 5 most commonly used devices for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "device_counts = pd.DataFrame(\n",
    "    [most_freq(tweets[name]['device']).rename(name)\n",
    "     for name in tweets]\n",
    ").fillna(0)\n",
    "device_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "make_bar_plot(device_counts.T, title=\"Count of Tweets by Source\",\n",
    "               xlabel=\"Source\", ylabel=\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Handle\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 2e\n",
    "\n",
    "What might we want to investigate further?  Write a few sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 2f\n",
    "\n",
    "We just looked at the top 5 most commonly used devices for each user. However, we used the number of tweets as a measure, when it might be better to compare these distributions by comparing _proportions_ of tweets. Why might proportions of tweets be better measures than numbers of tweets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "<br/><br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 3: When?\n",
    "\n",
    "Now that we've explored the sources of each of the tweets, we will perform some time series analysis. A look into the temporal aspect of the data could reveal insights about how a user spends their day, when they eat and sleep, etc. In this question, we will focus on the time at which each tweet was posted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3a\n",
    "\n",
    "Complete the following function `add_hour` that takes in a tweets dataframe `df`, and two column names `time_col` and `result_col`. Your function should use the timestamps in the `time_col` column to store in a new column `result_col` the computed  hour of the day as floating point number according to the formula:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^{2}}\n",
    "$$\n",
    "\n",
    "**Note:** The below code calls your `add_hour` function and updates each tweets dataframe by using the `created_at` timestamp column to calculate and store the `hour` column.\n",
    "\n",
    "**Hint:** See the following link for an example of working with timestamps using the [`dt` accessors](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def add_hour(df, time_col, result_col):\n",
    "    ...\n",
    "    return df\n",
    "\n",
    "# do not modify the below code\n",
    "tweets = {handle: add_hour(df, \"created_at\", \"hour\") for handle, df in tweets.items()}\n",
    "tweets[\"AOC\"][\"hour\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "With our new `hour` column, let's take a look at the distribution of tweets for each user by time of day. The following cell helps create a density plot on the number of tweets based on the hour they are posted. \n",
    "\n",
    "The function `bin_df` takes in a dataframe, an array of bins, and a column name; it bins the the values in the specified column, returning a dataframe with the bin lower bound and the number of elements in the bin. This function uses [`pd.cut`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html), a pandas [utility](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) for binning numerical values that you may find helpful in the distant future.\n",
    "\n",
    "Run the cell and answer the following question about the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "def bin_df(df, bins, colname):\n",
    "    binned = pd.cut(df[colname], bins).value_counts().sort_index()\n",
    "    return pd.DataFrame({\"counts\": binned, \"bin\": bins[:-1]})\n",
    "\n",
    "hour_bins = np.arange(0, 24.5, .5)\n",
    "binned_hours = {handle: bin_df(df, hour_bins, \"hour\") for handle, df in tweets.items()}\n",
    "\n",
    "make_line_plot(binned_hours, \"bin\", \"counts\", title=\"Distribution of Tweets by Time of Day\",\n",
    "               xlabel=\"Hour\", ylabel=\"Number of Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 3b\n",
    "Compare Cristiano's distribution with those of AOC and Elon Musk. In particular, compare the distributions before and after Hour 6. What differences did you notice? What might be a possible cause of that? Do the data plotted above seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 3c\n",
    "\n",
    "To account for different locations of each user in our analysis, we will next adjust the `created_at` timestamp for each tweet to the respective timezone of each user. Complete the following function `convert_timezone` that takes in a tweets dataframe `df` and a timezone `new_tz` and adds a new column `converted_time` that has the adjusted `created_at` timestamp for each tweet. The timezone for each user is provided in `timezones`.\n",
    "\n",
    "**Hint:** Again, please see the following link for an example of working with [`dt` accessors](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "convert-to-est",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_timezone(df, new_tz):\n",
    "    ...\n",
    "    return df\n",
    "\n",
    "timezones = {\"AOC\": \"EST\", \"Cristiano\": \"Europe/Lisbon\", \"elonmusk\": \"America/Los_Angeles\"}\n",
    "\n",
    "tweets = {handle: convert_timezone(df, tz) for (handle, df), tz in zip(tweets.items(), timezones.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "With our adjusted timestamps for each user based on their timezone, let's take a look again at the distribution of tweets by time of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "tweets = {handle: add_hour(df, \"converted_time\", \"converted_hour\") for handle, df in tweets.items()}\n",
    "binned_hours = {handle: bin_df(df, hour_bins, \"converted_hour\") for handle, df in tweets.items()}\n",
    "\n",
    "make_line_plot(binned_hours, \"bin\", \"counts\", title=\"Distribution of Tweets by Time of Day (timezone-corrected)\",\n",
    "               xlabel=\"Hour\", ylabel=\"Number of Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/><br/>\n",
    "<br/><br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 4: Sentiment\n",
    "\n",
    "\n",
    "In the past few questions, we have explored the sources of the tweets and when they are posted. Although on their own, they might not seem particularly intricate, combined with the power of regular expressions, they could actually help us infer a lot about the users. In this section, we will continue building on our past analysis and specifically look at the sentiment of each tweet -- this would lead us to a much more direct and detailed understanding of how the users view certain subjects and people. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "How do we actually measure the sentiment of each tweet? In our case, we can use the words in the text of a tweet for our calculation! For example, the word \"love\" within the sentence \"I love America!\" has a positive sentiment, whereas the word \"hate\" within the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of AOC's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "head-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the lexicon contains emojis too! Each row contains a word and the *polarity* of that word, measuring how positive or negative the word is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6a-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### VADER Sentiment Analysis\n",
    "\n",
    "The creators of [VADER](https://github.com/cjhutto/vaderSentiment#introduction) describe the tool’s assessment of polarity, or “compound score,” in the following way:\n",
    "\n",
    "“The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.”\n",
    "\n",
    "As you can see, VADER doesn't \"read\" sentences, but works by parsing sentences into words, assigning a preset generalized score from their testing sets to each word separately. \n",
    "\n",
    "VADER relies on humans to stabilize its scoring. The creators use Amazon Mechanical Turk, a crowdsourcing survey platform, to train its model. Its training data consists of a small corpus of tweets, New York Times editorials and news articles, Rotten Tomatoes reviews, and Amazon product reviews, tokenized using the natural language toolkit (NLTK). Each word in each dataset was reviewed and rated by at least 20 trained individuals who had signed up to work on these tasks through Mechanical Turk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 4a\n",
    "Please score the sentiment of one of the following words, using your own personal interpretation. No code is required for this question!\n",
    "\n",
    "- police\n",
    "- order\n",
    "- Democrat\n",
    "- Republican\n",
    "- gun\n",
    "- dog\n",
    "- technology\n",
    "- TikTok\n",
    "- security\n",
    "- face-mask\n",
    "- science\n",
    "- climate change\n",
    "- vaccine\n",
    "\n",
    "What score did you give it and why? Can you think of a situation in which this word would carry the opposite sentiment to the one you’ve just assigned?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**Optional (ungraded):** Are there circumstances (e.g. certain kinds of language or data) when you might not want to use VADER? What features of human speech might VADER misrepresent or fail to capture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4b\n",
    "\n",
    "Let's first load in the data containing all the sentiments. Read `vader_lexicon.txt` into a dataframe called `sent`. The index of the dataframe should be the words in the lexicon and should be named `token`. `sent` should have one column named `polarity`, storing the polarity of each word.\n",
    "\n",
    "**Hint:** The `pd.read_csv` function may help here. Since the file is tab-separated, be sure to set `sep='\\t'` in your call to `pd.read_csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent = ...\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6b-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### Question 4c\n",
    "\n",
    "Before further analysis, we will need some more tools that can help us extract the necessary information and clean our data.\n",
    "\n",
    "Complete the following regular expressions that will help us match part of a tweet that we either (i) want to remove or (ii) are interested in learning more about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4c Part (i)\n",
    "Assign a regular expression to a new variable `punct_re` that captures all of the punctuations within a tweet. We consider punctuation to be any non-word, non-whitespace character.\n",
    "\n",
    "**Note**: A word character is any character that is alphanumeric or an underscore. A whitespace character is any character that is a space, a tab, a new line, or a carriage return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "punct_re = r'...'\n",
    "\n",
    "re.sub(punct_re, \" \", tweets[\"AOC\"].iloc[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4c Part (ii)\n",
    "Assign a regular expression to a new variable `mentions_re` that matches any mention in a tweet. Your regular expression should use a capturing group to extract the user's username in a mention.\n",
    "\n",
    "**Hint**: a user mention within a tweet always starts with the `@` symbol and is followed by a series of word characters (with no space in between). For more explanations on what a word character is, check out the **Note** section in Part 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mentions_re = r'...'\n",
    "\n",
    "re.findall(mentions_re, tweets[\"AOC\"].iloc[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Tweet Sentiments and User Mentions\n",
    "\n",
    "As you have seen in the previous part of this question, there are actually a lot of interesting components that we can extract out of a tweet for further analysis! For the rest of this question though, we will focus on one particular case: the sentiment of each tweet in relation to the users mentioned within it. \n",
    "\n",
    "To calculate the sentiments for a sentence, we will follow this procedure:\n",
    "\n",
    "1. Remove the punctuation from each tweet so we can analyze the words.\n",
    "2. For each tweet, find the sentiment of each word.\n",
    "3. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4d\n",
    "\n",
    "Let's use our `punct_re` regular expression from the previous part to clean up the text a bit more! The goal here is to remove all of the punctuations to ensure words can be properly matched with those from VADER to actually calculate the full sentiment score.\n",
    "\n",
    "Complete the following function `sanitize_texts` that takes in a table `df` and adds a new column `clean_text` by converting all characters in its original `full_text` column to lower case and replace all instances of punctuations with a space character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_texts(df):\n",
    "    df[\"clean_text\"] = ...\n",
    "    return df\n",
    "\n",
    "tweets = {handle: sanitize_texts(df) for handle, df in tweets.items()}\n",
    "tweets[\"AOC\"][\"clean_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4e\n",
    "With the texts sanitized, we can now extract all the user mentions from tweets. \n",
    "\n",
    "Complete the following function `extract_mentions` that takes in the **`full_text`** (not `clean_text`!) column from a tweets dataframe  and uses `mentions_re` to extract all the mentions in a dataframe. The returned dataframe is:\n",
    "* single-indexed by the IDs of the tweets\n",
    "* has one row for each mention\n",
    "* has one column named `mentions`, which contains each mention in all lower-cased characters\n",
    "\n",
    "\n",
    "**Hint**: There are several ways to approach this problem. Here is documentation for potentially useful functions: `str.extractall` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extractall.html?highlight=extractall)) and `str.findall` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.findall.html)), `dropna` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html?highlight=dropna#pandas.Series.dropna)), and `explode` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html?highlight=series%20explode)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_mentions(full_texts):\n",
    "    mentions = ...\n",
    "    return mentions[[\"mentions\"]]\n",
    "\n",
    "# uncomment this line to help you debug\n",
    "display(extract_mentions(tweets[\"AOC\"][\"full_text\"]).head())\n",
    "\n",
    "# do not modify the below code\n",
    "mentions = {handle: extract_mentions(df[\"full_text\"]) for handle, df in tweets.items()}\n",
    "horiz_concat_df(mentions).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "\n",
    "### Tidying Up the Data\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. The `to_tidy_format` function implemented for you uses the `clean_text` column of each tweets dataframe to create a tidy table, which is:\n",
    "\n",
    "* single-indexed by the IDs of the tweets, for every word in the tweet.\n",
    "* has one column named `word`, which contains the individual words of each tweet.\n",
    "\n",
    "Run the following cell to convert the table into the tidy format. Take a look at the first 5 rows from the \"tidied\" tweets dataframe for AOC and see if you can find out how the structure has changed.\n",
    "\n",
    "**Note**: Although there is no work needed on your part, we have referenced a few more advanced pandas methods you might have not seen before -- you should definitely look them up in the documentation when you have a chance, as they are quite powerful in restructuring a dataframe into a useful intermediate state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "def to_tidy_format(df):\n",
    "    tidy = (\n",
    "        df[\"clean_text\"]\n",
    "        .str.split()\n",
    "        .explode()\n",
    "        .to_frame()\n",
    "        .rename(columns={\"clean_text\": \"word\"})\n",
    "    )\n",
    "    return tidy\n",
    "\n",
    "tidy_tweets = {handle: to_tidy_format(df) for handle, df in tweets.items()}\n",
    "tidy_tweets[\"AOC\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Adding in the Polarity Score\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "The following `add_polarity` function adds a new `polarity` column to the `df` table. The `polarity` column contains the sum of the sentiment polarity of each word in the text of the tweet.\n",
    "\n",
    "**Note**: Again, though there is no work needed on your part, it is important for you to go through how we set up this method and actually understand what each method is doing. In particular, see how we deal with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "def add_polarity(df, tidy_df):\n",
    "    df[\"polarity\"] = (\n",
    "        tidy_df\n",
    "        .merge(sent, how='left', left_on='word', right_index=True)\n",
    "        .reset_index()\n",
    "        .loc[:, ['id', 'polarity']]\n",
    "        .fillna(0)\n",
    "        .groupby('id')\n",
    "        .sum()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "tweets = {handle: add_polarity(df, tidy_df) for (handle, df), tidy_df in \\\n",
    "          zip(tweets.items(), tidy_tweets.values())}\n",
    "tweets[\"AOC\"][[\"clean_text\", \"polarity\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4f\n",
    "Finally, with our polarity column in place, we can finally explore how the sentiment of each tweet relates to the user(s) mentioned in it. \n",
    "\n",
    "Complete the following function `mention_polarity` that takes in a mentions dataframe `mentions` and the original tweets dataframe `df` and returns a series where the mentioned users are the index and the corresponding mean sentiment scores of the tweets mentioning them are the values.\n",
    "\n",
    "**Hint**: You should consider joining tables together in this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mention_polarity(df, mention_df):\n",
    "    ...\n",
    "\n",
    "aoc_mention_polarity = mention_polarity(tweets[\"AOC\"],mentions[\"AOC\"]).sort_values(ascending=False)\n",
    "aoc_mention_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 4g\n",
    "\n",
    "When grouping by mentions and aggregating the polarity of the tweets, what aggregation function should we use? What might be one drawback of using the mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "<br/><br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 5: You Do EDA!\n",
    "\n",
    "Congratulations! You have finished all of the preliminary analysis on AOC, Cristiano, and Elon Musk's recent tweets. \n",
    "\n",
    "As you might have recognized, there is still far more to explore within the data and build upon what we have uncovered so far. In this open-ended question, we want you to come up with a new perspective that can expand upon our analysis of the sentiment of each tweet. \n",
    "\n",
    "For this question, you will perform some text analysis on our `tweets` dataset. Your analysis should have two parts:\n",
    "\n",
    "1. a piece of code that manipulates `tweets` in some way and produces informative output (e.g. a dataframe, series, or plot)\n",
    "2. a short (4-5 sentence) description of the findings of your analysis: what were you looking for? What did you find? How did you go about answering your question?\n",
    "\n",
    "Your work should involve text analysis in some way, whether that's using regular expressions or some other form.\n",
    "\n",
    "To aid you in creating plots, we provide the plotting helper functions in the table below. These are same helpers we have used throughout this notebook, and all accept dictionaries with a similar structure to `tweets`. That being said, if you know how to make plots, please do so! Very soon in this class, you'll learn how to use the matplotlib and seaborn libraries that we use to write these the helpers.\n",
    "\n",
    "| Helper | Description |\n",
    "|--------|-------------|\n",
    "| `make_bar_plot` | Plot side-by-side bar plots of data like [`plt.bar`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.bar.html) |\n",
    "| `make_histogram` | Plot overlaid histograms of data like [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) |\n",
    "| `make_line_plot` | Plot overlaid line plots of data like [`plt.plot`](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.plot.html) |\n",
    "| `make_scatter_plot` | Plot overlaid scatter plots of data like [`plt.scatter`](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.scatter.html) |\n",
    "\n",
    "Each of the provided helpers is in `ds100_utils.py` and has a comprehensive docstring. You can read the docstring by calling `help` on the plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_line_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist you in getting started, here are a few ideas for this you can analyze for this question:\n",
    "\n",
    "- dig deeper into when devices were used\n",
    "- how sentiment varies with time of tweet\n",
    "- expand on regexes from 4b to perform additional analysis (e.g. hashtags)\n",
    "- examine sentiment of tweets over time\n",
    "\n",
    "In general, try to combine the analyses from earlier questions or create new analysis based on the scaffolding we have provided.\n",
    "\n",
    "This question is worth 4 points and will be graded based on this rubric:\n",
    "\n",
    "| | 2 points | 1 point | 0 points |\n",
    "|-----|-----|-----|-----|\n",
    "| **Code** | Produces a mostly informative plot or pandas output that addresses the question posed in the student's description and uses at least one of the following pandas DataFrame/Series methods: `groupby`, `agg`, `merge`, `pivot_table`, `str`, `apply` | Attempts to produce a plot or manipulate data but the output is unrelated to the proposed question, or doesn't utilize at least one of the listed methods | No attempt at writing code |\n",
    "| **Description** | Describes the analysis question and procedure comprehensively and summarizes results correctly | Attempts to describe analysis and results but description of results is incorrect or analysis of results is disconnected from the student’s original question | No attempt at writing a description |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 5a\n",
    "\n",
    "Use this space to put your EDA code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform your text analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Question 5b\n",
    "\n",
    "Use this space to put your EDA description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your description here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Homework 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "otter": {
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1a) == 1000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q1a.startswith(\"[{\\\"created_at\\\":\")\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> \"\\\"user\\\": {\\\"id\\\": 216776631\" in q1a\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q1b.lower() in ['A', 'B', 'C', 'D']\nFalse",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert set(tweets.keys()) == {\"AOC\", \"Cristiano\", \"elonmusk\"}\n>>> assert all(df.index.name == \"id\" for df in tweets.values())\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_cols = ['created_at', 'id_str', 'full_text', 'truncated', 'display_text_range',\n...        'entities', 'source', 'in_reply_to_status_id',\n...        'in_reply_to_status_id_str', 'in_reply_to_user_id',\n...        'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo',\n...        'coordinates', 'place', 'contributors', 'retweeted_status',\n...        'is_quote_status', 'retweet_count', 'favorite_count', 'favorited',\n...        'retweeted', 'lang', 'possibly_sensitive', 'extended_entities',\n...        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n...        'quoted_status']\n>>> all(col in df.columns for df in tweets.values() for col in expected_cols)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"AOC\"].shape[0] is not None) and (tweets[\"AOC\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"Cristiano\"].shape[0] is not None) and (tweets[\"Cristiano\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"elonmusk\"].shape[0] is not None) and (tweets[\"elonmusk\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([1333958991613923331,\n...  1312936075590074368,\n...  1248798917350883337,\n...  1204575943601328128,\n...  1346163629486387201,\n...  1246500923180036096,\n...  1315825309036564482,\n...  1255121845533229056,\n...  1352691282603364362,\n...  1243377508835381249])\n>>> set(np.random.choice(sorted(tweets[\"AOC\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([880473340619759616,\n...  174892632500219905,\n...  284705925351239680,\n...  403528698936041473,\n...  476770924587667456,\n...  697801434755133440,\n...  1214232898917801984,\n...  741648660476383233,\n...  48000397721337856,\n...  95524319232409600])\n>>> set(np.random.choice(sorted(tweets[\"Cristiano\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([1265750340613427200,\n...  1321275062998257665,\n...  1285430635088076800,\n...  1252987963329388544,\n...  1339567835840962560,\n...  1313512170336944128,\n...  1348823685445136385,\n...  1280090680430178305,\n...  1301647642846547971,\n...  1281695247626416129])\n>>> set(np.random.choice(sorted(tweets[\"elonmusk\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.sub(q2a_pattern, \"\", test_str) == 'Twitter for iPhone'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(q2a_pattern, \"\", \"<a bad='html'\") == \"<a bad='html'\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.findall(q2b_pattern, test_str) == ['Twitter for iPhone']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(q2b_pattern, \">findme<\") == ['findme']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(tweets.keys()) == {\"AOC\", \"Cristiano\", 'elonmusk'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(\"device\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> counts = tweets[\"AOC\"][\"device\"].value_counts().to_dict()\n>>> (set(counts.keys()) == {'Twitter Media Studio', 'Twitter for iPhone'}) and (set(counts.values()) == {2, 3245})\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(10202)\n>>> expected = ['Twitter for iPhone'] * 10\n>>> actual = []\n>>> for i in np.random.choice(sorted(tweets[\"AOC\"].index), replace=False, size=10):\n...     actual.append(tweets[\"AOC\"].loc[i, \"device\"])\n>>> actual == expected\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> most_freq(tweets[\"Cristiano\"]['device'])[\"Twitter for iPhone\"] == 1183\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> most_freq(tweets[\"AOC\"]['device'], k=1)[\"Twitter for iPhone\"] == 3245\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(\"hour\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(0 <= df[\"hour\"].min() and 24 >= df[\"hour\"].max() for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(\"converted_time\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((df[\"converted_time\"] == df[\"created_at\"]).all() for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(df[\"converted_time\"].dt.tz != df[\"created_at\"].dt.tz for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(sent, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sent.shape == (7517, 1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> tuple(sent.columns)  == (\"polarity\", )\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sent.index.name == \"token\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4ci": {
     "name": "q4ci",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(punct_re, str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(punct_re, \" \", \"a.b.c.1!2?3\")\n'a b c 1 2 3'",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4cii": {
     "name": "q4cii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.findall(mentions_re, \"@someone: this regex stuff is cool\") == ['someone']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(mentions_re, tweets[\"AOC\"].loc[1358149122264563712][\"full_text\"])[0] == \"RepEscobar\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> df = pd.DataFrame({\"full_text\": [\"a clean tweet\", \"an UPPPERcAsE tweet\", \"a ! tweet!!with..(*UF)punctuation\"]})\n>>> df = sanitize_texts(df)\n>>> df[\"clean_text\"].tolist() == ['a clean tweet', 'an upppercase tweet', 'a   tweet  with    uf punctuation']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(\"clean_text\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4e": {
     "name": "q4e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert all(not isinstance(df.index, pd.MultiIndex) for df in mentions.values())\n>>> assert all(set(df.columns) == {\"mentions\"} for df in mentions.values())\n>>> assert set(mentions.keys()) == {\"AOC\", \"Cristiano\", \"elonmusk\"}\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> horiz_mentions = horiz_concat_df(mentions)\n>>> horiz_mentions.loc[0][\"AOC\"][\"mentions\"] == 'repescobar'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> horiz_mentions = horiz_concat_df(mentions)\n>>> list(sorted(horiz_mentions.columns)) == [('AOC', 'mentions'), ('Cristiano', 'mentions'), ('elonmusk', 'mentions')]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4f": {
     "name": "q4f",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(aoc_mention_polarity.index) == set(mentions[\"AOC\"][\"mentions\"])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= aoc_mention_polarity.mean() <= 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
