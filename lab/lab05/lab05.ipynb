{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab05.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 5: Modeling, Loss Functions, and Summary Statistics\n",
    "\n",
    "**This assignment should be completed and submitted by Tuesday, February 22nd, 2022 at 11:59 PM PT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"V-PRSCA-3qc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *List names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Predicting Restaurant Tips\n",
    "\n",
    "In this lab, you will try to predict restaurant tips from a set of data in several ways:\n",
    "\n",
    "A. Without given any additional information, use a **constant model with L2 loss** to predict the tip $\\hat{y}$ as a summary statistic, $\\theta$.\n",
    "\n",
    "B. Given one piece of information—the total bill $x$—**use a linear model with L2 loss** to predict the tip $\\hat{y}$ as a linear function of $x$.\n",
    "\n",
    "C. See if a **constant model with L1 loss** changes our predictions.\n",
    "\n",
    "First, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick EDA**: Note that this dataset is likely from the United States. The below plot graphs the distribution of tips in this dataset, both in absolute amounts ($) and as a fraction of the total bill (post-tax, but pre-tip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "sns.histplot(tips['tip'], bins=20, stat=\"proportion\", ax=ax[0])\n",
    "sns.histplot(tips['tip']/tips['total_bill'], bins=20, stat=\"proportion\", ax=ax[1])\n",
    "ax[0].set_xlabel(\"Amount ($)\")\n",
    "ax[1].set_xlabel(\"Fraction of total bill\")\n",
    "ax[0].set_ylim((0, 0.35))\n",
    "ax[1].set_ylim((0, 0.35))\n",
    "ax[1].set_ylabel(\"\") # for cleaner visualization\n",
    "fig.suptitle(\"Restaurant Tips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll estimate the tip in **absolute amounts ($)**. The above plot is just to confirm your expectations about the `tips` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Part A: Tips as a Summary Statistic\n",
    "\n",
    "Let's first predict any restaurant tip using one single number: in other words, let's try to find the best statistic $\\hat{\\theta}$ to represent (i.e., **summarize**) the tips from our dataset.\n",
    "\n",
    "Each actual tip in our dataset is $y$, which is what we call the **observed value**. We want to predict each observed value as $\\hat{y}$. We'll save the observed tip values in a NumPy array `y_tips`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "y_tips = np.array(tips['tip']) # array of observed tips\n",
    "y_tips.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall the three-step process for modeling as covered in lecture:\n",
    "\n",
    "1. Define a **model.**\n",
    "\n",
    "1. Define a **loss function** and the associated **risk** on our training dataset (i.e., average loss).\n",
    "\n",
    "1. Find the best value of $\\theta$, known as $\\hat{\\theta}$, that **minimizes** risk.\n",
    "\n",
    "We'll go through each step of this process next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## A.1: Define the model\n",
    "\n",
    "We will define our model as the **constant model**:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta\n",
    "$$\n",
    "\n",
    "In other words, regardless of any other details (i.e., features) about their meal, we will always predict our tip $\\hat{y}$ as one single value: $\\theta$.\n",
    "\n",
    "\n",
    "$\\theta$ is what we call a **parameter**. Our modeling goal is to find the value of our parameter(s) that **best fit our data**.\n",
    "We have choice over which $\\theta$ we pick (using the data at hand), but ultimately we can only pick one to report, so we want to find the optimal parameter(s) $\\hat{\\theta}$.\n",
    "\n",
    "We call the constant model a **summary statistic**, as we are determining one number that best \"summarizes\" a set of values.\n",
    "\n",
    "\n",
    "No code to write here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## A.2: Define the loss function and risk\n",
    "\n",
    "Next, in order to pick our $\\theta$, we need to define a **loss function**, which is a measure of how well a model is able to predict the expected outcome. In other words, it measures the deviation of a predicted value $\\hat{y}$ from the observed value $y$.\n",
    "\n",
    "We will use **squared loss** (also known as the $L_2$ loss, pronounced \"ell-two\"). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_2$ loss of:\n",
    "\n",
    "$$\\Large L_2(y, \\hat{y}) = (y - \\hat{y})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "## Question 1\n",
    "\n",
    "In our constant model  $\\hat{y} = \\theta$, we always predict the tip as $\\theta$. Therefore our $L_2$ loss for some actual, observed value $y$ can be rewritten as:\n",
    "\n",
    "$$\\Large L_2(y, \\theta) = (y - \\theta)^2$$\n",
    "\n",
    "Use the function description below to implement the squared loss function for this single datapoint, assuming the constant model. Your answer should not use any loops.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_obs, theta):\n",
    "    \"\"\"\n",
    "    Calculate the squared loss of the observed data and a summary statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    y_obs: an observed value\n",
    "    theta : some constant representing a summary statistic\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The squared loss between the observation and the summary statistic.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "We just defined loss for a single datapoint. Let's extend the above loss function to our entire dataset by taking the **average loss** across the dataset.\n",
    "\n",
    "Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = \\{y_1, \\ldots, y_n\\}$, where $y_i$ is the $i^{th}$ tip (this is the `y_tips` array defined at the beginning of Part A).\n",
    "\n",
    "We can define the average loss (aka **risk**) over the dataset as:\n",
    "\n",
    "$$\\Large\n",
    "R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i})\n",
    "$$\n",
    "\n",
    "If we use $L_2$ loss per datapoint ($L = L_2$), then the risk is also known as **mean squared error** (MSE). For the constant model $\\hat{y}=\\theta$:\n",
    "\n",
    "$$\\Large\n",
    "R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Define the `mse_tips_constant` function which computes $R(\\theta)$ as the **mean squared error** on the tips data for a constant model with parameter $\\theta$.\n",
    "\n",
    "Notes/Hints:\n",
    "* This function takes in one parameter, `theta`; `data` is defined for you as a NumPy array that contains the observed tips values in the data.\n",
    "* Use the `squared_loss` function you defined in the previous question.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_tips_constant(theta):\n",
    "    data = y_tips\n",
    "    ...\n",
    "\n",
    "mse_tips_constant(5.3) # arbitrarily pick theta = 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## A.3: Find the $\\theta$ that minimizes risk\n",
    "\n",
    "## Question 3\n",
    "Now we can go about choosing our \"best\" value of $\\theta$, which we call $\\hat{\\theta}$, that minimizes our defined risk (which we defined as mean squared error). There are several approaches to computing $\\hat{\\theta}$ that we'll explore in this problem.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a: Visual Solution\n",
    "\n",
    "In the cell below  we plot the mean squared error for different thetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "theta_values = np.linspace(0, 6, 100)\n",
    "mse = [mse_tips_constant(theta) for theta in theta_values]\n",
    "plt.plot(theta_values, mse)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('average L2 loss')\n",
    "plt.title(r'MSE for different values of $\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Find the value of `theta` that minimizes the mean squared error via observation of the plot above. Round your answer to the nearest integer.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_observed_mse = ...\n",
    "min_observed_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Numerically computing $\\hat{\\theta}$\n",
    "\n",
    "`scipy.optimize.minimize` is a powerful method that can determine the optimal value of a variety of different functions. In practice, it is used to minimize functions that have no (or difficult to obtain) analytical solutions (it is a **numerical method**).\n",
    "\n",
    "It is overkill for our simple example, but nonetheless, we will show you how to use it, as it will become useful in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots some arbitrary 4th degree polynomial function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "x_values = np.linspace(-4, 2.5, 100)\n",
    "\n",
    "def fx(x):\n",
    "    return 0.1 * x**4 + 0.2*x**3 + 0.2 * x **2 + 1 * x + 10\n",
    "\n",
    "plt.plot(x_values, fx(x_values));\n",
    "plt.title(\"Arbitrary 4th degree polynomial\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the plot, we see that the x that minimizes the function is slightly larger than -2. What if we want the exact value? We will demonstrate how to grab the minimum value and the optimal `x` in the following cell.\n",
    "\n",
    "The function `minimize` from [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) will attempt to minimize any function you throw at it. Try running the cell below, and you will see that `minimize` seems to get the answer correct.\n",
    "\n",
    "Note: For today, we'll let `minimize` work as if by magic. We'll discuss how `minimize` works later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "from scipy.optimize import minimize\n",
    "minimize(fx, x0 = 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:<br/>\n",
    "[1] `fun`: the minimum value of the function. <br/>\n",
    "[2] `x`: the x which minimizes the function. We can index into the object returned by `minimize` to get these values. We have to add the additional `[0]` at the end because the minimizing x is returned as an array, but this is not necessarily the case for other attributes (i.e. `fun`), shown in the cell below.\n",
    "\n",
    "Note [2] means that `minimize` can also minimize multivariable functions, which we'll see in the second half of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "min_result = minimize(fx, x0 = 1.1)\n",
    "\n",
    "min_of_fx = min_result['fun']\n",
    "x_which_minimizes_fx = min_result['x'][0]\n",
    "min_of_fx, x_which_minimizes_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial guess**: The parameter `x0` that we passed to the `minimize` function is where the `minimize` function starts looking as it tries to find the minimum. For example, above, `minimize` started its search at x = 1.1 because that's where we told it to start. For the function above, it doesn't really matter what x we start at because the function is nice and has only a single local minimum. More technically, the function is nice because it is [convex](https://en.wikipedia.org/wiki/Convex_function), a property of functions that we will discuss later in the course.\n",
    "\n",
    "**Local minima**: `minimize` isn't perfect. For example, if we give it a function with many valleys (also known as local minima) it can get stuck. For example, consider the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "w_values = np.linspace(-2, 10, 100)\n",
    "\n",
    "def fw(w):\n",
    "    return 0.1 * w**4 - 1.5*w**3 + 6 * w **2 - 1 * w + 10\n",
    "\n",
    "plt.plot(w_values, fw(w_values));\n",
    "plt.title(\"Arbitrary function with local minima\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start the minimization at `w` = `6.5`, we'll get stuck in the local minimum at w = `7.03`. Note that no matter what your actual variable is called in your function (`w` in this case), the `minimize` routine still expects a starting point parameter called `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "minimize(fw, x0 = 6.5)    # initial w is 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3b: Numerical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using the `minimize` function, find the value of `theta` that minimizes the mean squared error for our `tips` dataset. In other words, you want to find the exact minimum of the plot that you saw in the previous part.\n",
    "\n",
    "Notes: \n",
    "* You should use the function you defined earlier: `mse_tips_constant`.\n",
    "* For autograding purposes, assign `min_scipy` to the value of `theta` that minimizes the MSE according to the `minimize` function, called with initial `x0=0.0`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call minimize with initial x0 = 0.0\n",
    "min_scipy = ...\n",
    "min_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 3c: Analytical Solution\n",
    "\n",
    "In lecture, we used calculus to show that the value of theta that minimizes the mean squared error for the constant model is the average (mean) of the data. Assign `min_computed` to the mean of the observed `y_tips` data, and compare this to the values you observed in questions 3a and 3b.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_computed = ...\n",
    "min_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Reflecting on the lab so far, we used a 3-step approach to find the \"best\" summary statistic $\\theta$:\n",
    "\n",
    "1. Define the constant model $\\hat{y}=\\theta$.\n",
    "\n",
    "1. Define \"best\": Define loss per datapoint (L2 loss) and consequently define risk $R(\\theta)$ over a given data array as the mean squared error, or MSE.\n",
    "\n",
    "1. Find the $\\theta = \\hat{\\theta}$ that minimizes the MSE $R(\\theta)$ in several ways:\n",
    "\n",
    "    * Visually: Create a plot of $R(\\theta)$ vs. $\\theta$ and eyeball the minimizing $\\hat{\\theta}$.\n",
    "    * Numerically: Create a function that returns $R(\\theta)$, the MSE for the given data array for a given $\\theta$, and use the scipy minimize function to find the minimizing $\\hat{\\theta}$.\n",
    "    * Analytically: Simply compute $\\hat{\\theta}$ the mean of the given data array, since this minimizes the defined $R(\\theta)$.\n",
    "    * (a fourth analytical option) Use calculus to find $\\hat{\\theta}$ that minimizes MSE $R(\\theta)$.\n",
    "    \n",
    "At this point, you've hopefully convinced yourself that the mean of the data is the summary statistic that minimizes mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our prediction for every meal's tip**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "def predict_tip_constant():\n",
    "    return min_computed\n",
    "\n",
    "# do not edit below this line\n",
    "bill = 20\n",
    "print(f\"\"\"No matter what meal you have, Part A's modeling process\n",
    "    predicts that you will pay a tip of ${predict_tip_constant():.2f}.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Part B: Tips as a Linear Function of Total Bill\n",
    "\n",
    "In this section, you will follow the exact same modeling process but instead use total bill to predict tip.\n",
    "\n",
    "We'll save the observed total bill values (post-tax but pre-tip) and the observed tip values in two NumPy arrays, `x_total_bills` and `y_tips`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "x_total_bills = np.array(tips['total_bill']) # array of total bill amounts\n",
    "y_tips = np.array(tips['tip'])              # array of observed tips\n",
    "print(\"total bills\", x_total_bills.shape)\n",
    "print(\"tips\", y_tips.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## B.1 Define the model\n",
    "\n",
    "We will define our model as the **linear model** that takes a single input feature, `total_bill` ($x$):\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = a + b x\n",
    "$$\n",
    "\n",
    "Our \"parameter\" $\\theta$ is actually two parameters: $a$ and $b$. You may see this written as $\\theta = (a, b)$.\n",
    "\n",
    "Our modeling task is then to pick the best values $a = \\hat{a}$ and $b = \\hat{b}$ from our data. Then, given the total bill $x$, we can predict the tip as $\\hat{y} = \\hat{a} + \\hat{b} x$.\n",
    "\n",
    "No code to write here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## B.2: Define the loss function and risk\n",
    "\n",
    "Next, we'll define our loss function $L(y, \\hat{y})$ and consequently our risk function $R(\\theta) = R(a, b)$.\n",
    "\n",
    "Similar to our approach to Part A, we'll use L2 Loss and Mean Squared Error. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, where $(x_i, y_i)$ are the $i^{th}$ total bill and tip, respectively, in our dataset.\n",
    "\n",
    "Our L2 Loss and Mean Squared Error are therefore:\n",
    "\n",
    "\\begin{align}\n",
    "\\large L_2(y, \\hat{y}) = \\large (y - \\hat{y})^2 &= \\large (y - (a + bx))^2 \\\\\n",
    "\\large R(a, b) = \\large \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) &= \\large  \\frac{1}{n} \\sum_{i = 1}^n(y_i - (a + b x_i))^2\n",
    "\\end{align}\n",
    "\n",
    "Notice that because our model is now the linear model $\\hat{y} = a + bx$, our final expressions for Loss and MSE are different from Part A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Define the `mse_tips_linear` function which computes $R(a, b)$ as the **mean squared error** on the tips data for a linear model with parameters $a$ and $b$.\n",
    "\n",
    "Notes:\n",
    "* This function takes in two parameters `a` and `b`.\n",
    "* You should use the NumPy arrays `x_total_bills` and `y_tips` defined at the beginning of Part B.\n",
    "* We've included some skeleton code, but feel free to write your own as well.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_tips_linear(a, b):\n",
    "    \"\"\"\n",
    "    Returns average L2 loss between\n",
    "    predicted y_hat values (using x_total_bills and parameters a, b)\n",
    "    and actual y values (y_tips)\n",
    "    \"\"\"\n",
    "    y_hats = ...\n",
    "    ...\n",
    "\n",
    "mse_tips_linear(5, 0) # arbitrarily pick a = 0.9, b = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## B.3: Find the $\\theta$ that minimizes risk\n",
    "\n",
    "Similar to before, we'd like to try out different approaches to finding the optimal parameters $\\hat{a}$ and $\\hat{b}$ that minimize MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5: Analytical Solution\n",
    "\n",
    "In lecture, we derived the following expression for the line of best fit:\n",
    "\n",
    "$$\\Large \\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$$\n",
    "\n",
    "where $\\bar{x}$, $\\bar{y}$, $SD(x)$, $SD(y)$ correspond to the means and standard deviations of $x$ and $y$, respectively, and $r$ is the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5a\n",
    "\n",
    "Assign `x_bar`, `y_bar`, `std_x`, `std_y`, and `r`, for our dataset. Note: Make sure to use `np.std`, and not `<Series name>.std()`.\n",
    "\n",
    "- Hint: Remember, in our case, `y` is `y_tips`, and `x` is `x_total_bills`.\n",
    "- Hint: You may find `np.corrcoef` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)) handy in computing `r`. Note that the output of `np.corrcoef` is a matrix, not a number, so you'll need to collect the correlation coefficient by indexing into the returned matrix.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = ...\n",
    "y_bar = ...\n",
    "std_x = ...\n",
    "std_y = ...\n",
    "r = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now, set `b_hat` and `a_hat` correctly, in terms of the variables you defined above. \n",
    "\n",
    "Hints:\n",
    "* Try and match the slope and intercept in $\\hat{y_i} = \\hat{a} + \\hat{b}x_i$ to the slope and intercept in $\\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$.\n",
    "* You may want to define `a_hat` in terms of `b_hat`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hat = ...\n",
    "a_hat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Question 5c\n",
    "\n",
    "Now, use `a_hat` and `b_hat` to implement the `predict_tip_linear` function, which predicts the tip for a total bill amount of `bill`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tip_linear(bill):\n",
    "    ...\n",
    "\n",
    "# do not edit below this line\n",
    "bill = 20\n",
    "print(f\"\"\"If you have a ${bill} bill, Part B's modeling process\n",
    "    predicts that you will pay a tip of ${predict_tip_linear(bill):.2f}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Numerically computing $\\hat{\\theta}$\n",
    "The `minimize` function we introduced earlier can also minimize functions of multiple variables (useful for numerically computing $\\hat{a}$ and $\\hat{b}$. There's one quirk, however, which is that the function has to accept its parameters as a single list.\n",
    "\n",
    "For example, consider the multivariate $f(u, v) = u^2 - 2 u v - 3 v + 2 v^2$. It turns out this function's minimum is at $(1.5, 1.5)$. To minimize this function, we create `f`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "def f(theta):\n",
    "    u = theta[0]\n",
    "    v = theta[1]\n",
    "    return u**2 - 2 * u * v - 3 * v + 2 * v**2\n",
    "\n",
    "minimize(f, x0 = [0.0, 0.0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Question 6: Numerical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Implement the `mse_tips_linear_list` function, which is exactly like `mse_tips_linear` defined in Question 4 except that it takes in a single list of 2 variables rather than two separate variables. For example `mse_tips_linear_list([2, 3])` should return the same value as `mse_tips_linear(2, 3)`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_tips_linear_list(theta):\n",
    "    \"\"\"\n",
    "    Returns average L2 loss between\n",
    "    predicted y_hat values (using x_total_bills and linear params theta)\n",
    "    and actual y values (y_tips)\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 6b\n",
    "\n",
    "Now, set `min_scipy_linear` to the result of calling minimize to optimize the risk function you just implemented.\n",
    "\n",
    "Hint: Make sure to set `x0`, say, to `[0.0, 0.0]`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call minimize with initial x0 = [0.0, 0.0]\n",
    "min_scipy_linear = ...\n",
    "min_scipy_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above output from your call to `minimize`, running the following cell will set and print the values of `a_hat` and `b_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "a_hat_scipy = min_scipy_linear['x'][0]\n",
    "b_hat_scipy = min_scipy_linear['x'][1]\n",
    "a_hat_scipy, b_hat_scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "The following cell will print out the values of `a_hat` and `b_hat` computed from both methods (\"manual\" refers to the analytical solution in Question 5; \"scipy\" refers to the numerical solution in Question 6). If you've done everything correctly, these should be very close to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "print('a_hat_scipy: ', a_hat_scipy)\n",
    "print('a_hat_manual: ', a_hat)\n",
    "print('\\n')\n",
    "print('b_hat_scipy: ', b_hat_scipy)\n",
    "print('b_hat_manual: ', b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "**Visual Solution** (not graded): Feel free to interact with the below plot and verify that the $\\hat{a}$ and $\\hat{b}$ you computed using either method above minimize the MSE. In the cell below  we plot the mean squared error for different parameter values. Note that now that we have two parameters, we have a 3D MSE surface plot.\n",
    "\n",
    "Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure. If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "a_values = np.linspace(-1, 1, 80)\n",
    "b_values = np.linspace(-1,1, 80)\n",
    "mse_values = [mse_tips_linear(a, b) \\\n",
    "              for a, b in itertools.product(a_values, b_values)]\n",
    "mse_values = np.reshape(mse_values, (len(a_values), len(b_values)), order='F')\n",
    "fig = go.Figure(data=[go.Surface(x=a_values, y=b_values, z=mse_values)])\n",
    "fig.update_layout(\n",
    "    title=r'MSE for different values of $a, b$',\n",
    "    autosize=False,\n",
    "    scene = dict(\n",
    "                    xaxis_title='x=a',\n",
    "                    yaxis_title='y=b',\n",
    "                    zaxis_title='z=MSE'),\n",
    "                    width=500,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing Constant Model vs Linear Model\n",
    "At this point, we can actually compare our two models! Both the linear model and constant model were optimized using the same L2 loss function but predict different values for different tips.\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = x_total_bills, y = y_tips, label='observed');\n",
    "\n",
    "# the below plot expects you've run all of Question 5\n",
    "plt.plot(x_total_bills, predict_tip_linear(x_total_bills), label='linear', color='g');\n",
    "\n",
    "# the below function expects you've run the cell right before part B\n",
    "plt.axhline(y=predict_tip_constant(), label='constant', color='m', ls='--');\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"total bill\")\n",
    "plt.ylabel(\"tip\")\n",
    "plt.title(\"Tips: Linear vs Constant Models\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while we plot tip by total bill, the constant model doesn't use the total bill in its prediction and therefore shows up as a horizontal line.\n",
    "\n",
    "**Thought question**: For predicting tip on this data, would you rather use the constant model or the linear model, assuming an L2 loss function for both? This might be more fun with a partner. Note, your answer will not be graded, so don't worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Part C: Using a Different Loss Function\n",
    "\n",
    "In this last (short) section, we'll consider how the optimal parameters for the **constant model** would change if we used a different loss function.\n",
    "\n",
    "\n",
    "We will now use **absolute loss** (also known as the $L_1$ loss, pronounced \"ell-one\"). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_1$ loss of:\n",
    "\n",
    "$$\\Large L_1(y, \\hat{y}) = |y - \\hat{y}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "While we still define **risk** as **average loss**, since we now use $L_1$ loss per datapoint in our datset $\\mathcal{D} = \\{y_1, \\ldots, y_n\\}$, our risk is now known as **mean absolute error** (MAE).\n",
    "\n",
    "For the constant model $\\hat{y} = \\theta$ (i.e., we predict our tip as a summary statistic):\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\Large R\\left(\\theta\\right) &= \\Large \\frac{1}{n} \\sum_{i=1}^n L_1(y_i, \\hat{y_i}) \\\\\n",
    "&= \\Large \\frac{1}{n} \\sum_{i=1}^n |y_i - \\theta|\n",
    "\\end{align}\n",
    "\n",
    "Note: the last line results from using the constant model for $\\hat{y}$. If we decided to use the linear model, we would have a different expression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Question 7\n",
    "\n",
    "### Question 7a\n",
    "\n",
    "Define the `mae_tips_constant` function which computes $R(\\theta)$ as the **mean absolute error** (MAE) on the tips data for a constant model with parameter $\\theta$.\n",
    "\n",
    "Hint: You may want to check out your solution from Question 2, which computed mean squared error (MSE).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_tips_constant(theta):\n",
    "    data = y_tips\n",
    "    ...\n",
    "\n",
    "mae_tips_constant(5.3) # arbitrarily pick theta = 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7b\n",
    "\n",
    "In lecture, we saw that the value of theta that minimizes mean *absolute* error for the constant model is the median of the data. Assign `min_computed_mae` to the median of the observed `y_tips` data.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_computed_mae = ...\n",
    "min_computed_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Comparing MAE to MSE\n",
    "\n",
    "Now run the below cell to compare MAE with MSE on the constant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "fig, ax = plt.subplots(nrows=2, figsize=((6, 8)))\n",
    "theta_values = np.linspace(0, 6, 100)\n",
    "mse = [mse_tips_constant(theta) for theta in theta_values]\n",
    "ax[0].plot(theta_values, mse)\n",
    "ax[0].axvline(x=min_computed, linewidth=4, color='k', ls='--',\n",
    "              label=r'$\\hat{\\theta}$')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel(\"avg L2 loss (MSE)\")\n",
    "\n",
    "mae = [mae_tips_constant(theta) for theta in theta_values]\n",
    "ax[1].plot(theta_values, mae, color='orange')\n",
    "ax[1].axvline(x=min_computed_mae, linewidth=4, color='k', ls='--',\n",
    "              label=r'$\\hat{\\theta}$')\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel(\"avg L1 loss (MAE)\")\n",
    "\n",
    "ax[1].set_xlabel(r'$\\theta$');\n",
    "fig.suptitle(r\"MAE vs MSE (constant model) for different values of $\\theta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought question** You should see that the MAE plot (below) looks somewhat similar the MSE plot (above). Try to identify any key differences you observe and write them down below. This might be more fun with a partner. Note, your answer will not be graded, so don't worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You finished the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Our Observations on Constant Model vs Linear Model\n",
    "\n",
    "Earlier in this lab, we said we'd describe our observations about whether to use Constant Model or Linear Model (both trained with MSE). Here are some thoughts:\n",
    "\n",
    "Recall that $r$ is the correlation coefficient, where values closer to -1 or 1 imply a very linear relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you computed this in Q5a\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $x$ and $y$ is *somewhat* linear; you can see this more clearly through the scatter plot, where there are many points that don't fall close to the linear model line. With this in mind:\n",
    "\n",
    "\n",
    "* The linear model seems to work well for most bills.\n",
    "* However, as bills get bigger, some datapoints seem to suggest that the constant model works better.\n",
    "* In the wild, a tip predictor may use a combination of both the constant and linear models we trained: an average prediction, or a random coin flip to pick the model, or some heuristic decision to choose one model if the total bill exceeds a certain threshold.\n",
    "\n",
    "In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. You will have an opportunity to explore your own models in a future assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Our Observations on Differences Between MAE vs. MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in this lab, we said we'd describe our observations about the differences between the MAE and MSE.\n",
    "\n",
    "There are three key differences that we identified between the plots of the MSE and MAE.\n",
    "\n",
    "1. The minimizing $\\theta = \\hat{\\theta}$ is different.\n",
    "2. The plot for MAE increases linearly instead of quadratically as we move far away from the minimizing $\\theta$.\n",
    "3. The plot for MAE is piecewise linear instead of smooth. Each change in slope happens at the same $\\theta$ value as a data point in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "301px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
