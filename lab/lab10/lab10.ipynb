{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab10.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 10: Feature Engineering & Cross-Validation\n",
    "In this lab, you will practice using `scikit-learn` to do feature engineering and cross-validation to produce a model with low error on held-out data.\n",
    "\n",
    "### Due Date \n",
    "This assignment is due on **Thursday, November 4th at 11:59pm PDT**.\n",
    "\n",
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from IPython.display import display, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "For this lab, we will use a toy dataset to predict the house prices in Boston with data provided by the `sklearn.datasets` package. There are more interesting datasets in the package if you want to explore them during your free time!\n",
    "\n",
    "Run the following cell to load the data. `load_boston()` will return a dictionary object which includes keys for:\n",
    "- `data` : the covariates (X)\n",
    "- `target` : the response vector (Y)\n",
    "- `feature_names`: the column names\n",
    "- `DESCR` : a full description of the data\n",
    "- `filename`: name of the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load_data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_data = load_boston()\n",
    "print(boston_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "data_description",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "A look at the `DESCR` attribute tells us the data contains these features:\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 USD\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of black \n",
    "                 residents by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    \n",
    "Let's now convert this data into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "data_head",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Let's model this housing price data! Before we can do this, however, we need to split the data into training and test sets. Remember that the response vector (housing prices) lives in the `target` attribute. A random seed is set here so that we can deterministically generate the same splitting in the future if we want to test our result again and find potential bugs.\n",
    "\n",
    "Use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split out 10% of the data for the test set. Call the resulting splits `X_train`, `X_test`, `Y_train`, `Y_test`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(47)\n",
    "\n",
    "X = boston\n",
    "Y = pd.Series(boston_data['target'])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "As a warmup, fit a linear model to describe the relationship between the housing price and all available covariates. We've imported `sklearn.linear_model` as `lm`, so you can use that instead of typing out the whole module name. Fill in the cells below to fit a linear regression model to the covariates and create a scatter plot for our predictions vs. the true prices.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "linear_model = lm.LinearRegression()\n",
    "\n",
    "# Fit your linear model\n",
    "#linear_model.fit(...)\n",
    "\n",
    "# Predict housing prices on the test set\n",
    "Y_pred = ...\n",
    "\n",
    "# Plot predicted vs true prices\n",
    "plt.scatter(Y_test, Y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Prices vs Predicted Prices\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-655458f2b7de0645",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Briefly analyze the scatter plot above. Do you notice any outliers? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "As we find from the scatter plot, our model is not perfect. If it were perfect, we would see the identity line (i.e. a line of slope 1). Compute the root mean squared error (RMSE) of the predicted responses: \n",
    "\n",
    "$$\n",
    "\\textbf{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 }\n",
    "$$\n",
    "\n",
    "Fill out the function below and compute the RMSE for our predictions on both the training data `X_train` and the test set `X_test`.  Your implementation **should not** use for loops.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def rmse(actual_y, predicted_y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predicted_y: an array of the prediction from the model\n",
    "        actual_y: an array of the groudtruth label\n",
    "        \n",
    "    Returns:\n",
    "        The root mean square error between the prediction and the groudtruth\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "train_error = ...\n",
    "test_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", train_error)\n",
    "print(\"Test RMSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f349e0d791db2f2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Is your training error lower than the test error? If so, why could this be happening? Answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can get even higher accuracy by adding more features. For example, the code below adds the square, square root, and hyperbolic tangent of every feature to the design matrix. We've chosen these bizarre features specifically to highlight overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_with_extra_features = boston.copy()\n",
    "for feature_name in boston.columns:\n",
    "    boston_with_extra_features[feature_name + \"^2\"] = boston_with_extra_features[feature_name] ** 2\n",
    "    boston_with_extra_features[\"sqrt\" + feature_name] = np.sqrt(boston_with_extra_features[feature_name])\n",
    "    boston_with_extra_features[\"tanh\" + feature_name] = np.tanh(boston_with_extra_features[feature_name])\n",
    "    \n",
    "boston_with_extra_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split up our data again and refit the model. From this cell forward, we append `2` to the variable names `X_Train, X_test, Y_train, Y_test, train_error, test_error` in order to maintain our original data. **Make sure you use these variable names from this cell forward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(32)\n",
    "X = boston_with_extra_features\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X, Y, test_size = 0.10)\n",
    "linear_model.fit(X_train2, Y_train2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our training and test RMSE, we see that they are lower than you computed earlier. This strange model is seemingly better, even though it includes seemingly useless features like the hyperbolic tangent of the average number of rooms per dwelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_error2 = rmse(Y_train2, linear_model.predict(X_train2)) \n",
    "test_error2 = rmse(Y_test2, linear_model.predict(X_test2))\n",
    "\n",
    "print(\"Training RMSE:\", train_error2)\n",
    "print(\"Test RMSE:\", test_error2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we plot the training and test error as we add each additional feature, we see that our model is beginning to overfit. That is, even though our training error continues to decrease (since our model bias is decreasing), our test error starts increasing since the lessons learned from these last 20+ features aren't actually useful when applied to unseen data. That is, our model isn't generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_vs_N = []\n",
    "test_error_vs_N = []\n",
    "\n",
    "range_of_num_features = range(1, X_train2.shape[1] + 1)\n",
    "\n",
    "for N in range_of_num_features:\n",
    "    X_train_first_N_features = X_train2.iloc[:, :N]    \n",
    "    \n",
    "    linear_model.fit(X_train_first_N_features, Y_train2)\n",
    "    train_error_overfit = rmse(Y_train2, linear_model.predict(X_train_first_N_features))\n",
    "    train_error_vs_N.append(train_error_overfit)\n",
    "    \n",
    "    X_test_first_N_features = X_test2.iloc[:, :N]\n",
    "    test_error_overfit = rmse(Y_test2, linear_model.predict(X_test_first_N_features))    \n",
    "    test_error_vs_N.append(test_error_overfit)\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_of_num_features, train_error_vs_N)\n",
    "plt.plot(range_of_num_features, test_error_vs_N)\n",
    "plt.legend([\"training\", \"test error\"])\n",
    "plt.title('RMSE vs Number of Features')\n",
    "plt.xlabel(\"number of features\")\n",
    "plt.ylabel(\"RMSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cv",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "To figure out which feature set to use, we can use cross validation. Since there are 52 features, in theory, we could consider $2^{52}$ different models. For the sake of simplicity, we'll consider only 52 models, where the $i$th model includes features 1 through $i$. For example, the 3rd model would include the first 3 features only (\"CRIM\", \"ZN\", and \"INDUS\").\n",
    "\n",
    "While using fewer features may increase our training error, it may also decrease our test error and help prevent overfitting to the training set.\n",
    "\n",
    "You might ask, why don't we just use the plot above to find $p$, the optimal number of features? In real life, you'd NEVER generate this plot. Using the test set more than once for any reason is not a good idea. Otherwise, you're effectively using the test data to fit your hyperparameters (in this case, N, the number of features).\n",
    "\n",
    "In the next section, we'll instead use $k$-fold cross-validation to select the best subset of features for our model. Recall the approach looks something like:\n",
    "\n",
    "<img src=\"cv.png\" width=500px>\n",
    "\n",
    "**Warning**: The above questions are to illustrate some general differences we can see between train set and test set and the phenomenon on overfitting, however, we **should not use the test set at all when selecting models**. Instead, we should use cross-validation to select the feature set. When selecting features or choosing hyper-parameters, we can split the training set further into multiple train and validation sets (each split is called a \"fold\", hence k-fold cross validation). We will use the average validation error across all k folds to help select the optimal hyper-parameters and feature set.\n",
    "\n",
    "**Furthermore,** selecting the first $N$ features, in general, is not a good way to select features, since the order of our columns is relatively meaningless. We present this as an example for you here to illustrate the concept of cross validation. In future assignments, we will look at more robust methods for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Scikit-learn has built-in support for cross validation.  However, to better understand how cross validation works complete the following function which cross validates a given model.\n",
    "\n",
    "1. Use the [`KFold.split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function to get 4 splits on the training data. Note that `split` returns the indices of the data for that split.\n",
    "2. For **each** split:\n",
    "    1. Select out the training and validation rows and columns based on the split indices and features.\n",
    "    2. Compute the RMSE on the validation split.\n",
    "    3. Return the average error across all cross validation splits.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def compute_CV_error(model, X_train, Y_train):\n",
    "    '''\n",
    "    Split the training data into 4 subsets.\n",
    "    For each subset, \n",
    "        fit a model holding out that subset\n",
    "        compute the MSE on that subset (the validation set)\n",
    "    You should be fitting 4 models total.\n",
    "    Return the average MSE of these 4 folds.\n",
    "\n",
    "    Args:\n",
    "        model: an sklearn model with fit and predict functions \n",
    "        X_train (data_frame): Training data\n",
    "        Y_train (data_frame): Label \n",
    "\n",
    "    Return:\n",
    "        the average validation MSE for the 4 splits.\n",
    "    '''\n",
    "    kf = KFold(n_splits=4)\n",
    "    validation_errors = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        # split the data\n",
    "        split_X_train, split_X_valid = ..., ...\n",
    "        split_Y_train, split_Y_valid = ..., ...\n",
    "\n",
    "        # Fit the model on the training split\n",
    "        ...\n",
    "        \n",
    "        # Compute the RMSE on the validation split\n",
    "        error = ...\n",
    "\n",
    "\n",
    "        validation_errors.append(error)\n",
    "        \n",
    "    return np.mean(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-60cbde80f3e2acc4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Use `compute_CV_error` to determine how many of the first $N$ features we should use to get the lowest average validation error. Then, fill in the variables `best_num_features`, `best_err`.\n",
    "\n",
    "**Hint:** To find the index of the lowest error in `errors`, you may want to use [`np.argmin`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html). Watch out for off by one issues when converting the index to the number of features!\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for N in range_of_num_features:\n",
    "    print(f\"Trying first {N} features\")\n",
    "    model = lm.LinearRegression()\n",
    "    \n",
    "    # compute the cross validation error\n",
    "    error = ...\n",
    "    \n",
    "    print(\"\\tRMSE:\", error)\n",
    "    errors.append(error)\n",
    "\n",
    "best_num_features = ...\n",
    "best_err = ...\n",
    "\n",
    "print(f\"Best choice, use the first {best_num_features} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f5a870c74e96a0c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Why is it logical to use the set of features that result in the smallest average root mean squared error when performing cross-validation? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Finally, fit a linear model using your best feature set and predict housing prices for your original test set. You can also try to select your own features (on top of the given ones) to lower the RMSE. Compute the final train and test RMSEs for a linear model using your best feature set.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Fit your linear model\n",
    "...\n",
    "\n",
    "# Predict points from our test set and calculate the rmse\n",
    "train_rmse = ... \n",
    "test_rmse = ...\n",
    "\n",
    "print(\"Train RMSE\", train_rmse)\n",
    "print(\"KFold Validation RMSE\", best_err)\n",
    "print(\"Test RMSE\", test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1073b5a4c1d25928",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Here we've plotted a residual plot for each record from `X_test2`. After seeing your testing and training error, it is often helpful to visiualize your error. When points in the residual plot are randomly scattered around the line y = 0, then we know that a linear regression model is good for the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69418d5b2a92f393",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "fitted_values = model.predict(X_test2.iloc[:, :best_num_features])\n",
    "plt.scatter(fitted_values, Y_test2 - fitted_values)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('residual (true y - estimated y)')\n",
    "plt.title(\"Residual of prediction for i'th house\")\n",
    "plt.axhline(y = 0, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cv_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Nice! You've used $k$-fold cross-validation to fit a linear regression model to the housing data.\n",
    "\n",
    "In the future, you'd probably want to use something like [`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) to automatically perform cross-validation, but it's instructive to do it yourself at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "Congratulations! You are finished with this assignment. Please don't forget to submit by 11:59pm PST on Thursday, November 4th!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
