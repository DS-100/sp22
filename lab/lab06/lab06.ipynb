{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Linear Regression\n",
    "\n",
    "**This assignment should be completed and submitted by Tuesday, March 1st at 11:59 PM PT.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"IkkhAr3e19Q\", list = 'PLQCcNQgUcDfpuwnASdUyvQky51ZcYMWSy', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *List names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will review the details of linear regresison as described in Lectures 10 and 11. In particular:\n",
    "\n",
    "* Matrix formulation and solution to Ordinary Least Squares\n",
    "* `sns.lmplot` as a quick visual for simple linear regression\n",
    "* `scikit-learn`, a real world data science tool that is more robust and flexible than analytical/`scipy.optimize` solutions\n",
    "\n",
    "You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression.\n",
    "\n",
    "<br/>\n",
    "\n",
    "For the first part of this lab, you will predict fuel efficiency (`mpg`) of several models of automobiles using a **single feature**: engine power (`horsepower`). For the second part, you will perform feature engineering on **multiple features** to better predict fuel efficiency.\n",
    "\n",
    "First, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we load the fuel dataset, and drop any rows that have missing data\n",
    "vehicle_data = sns.load_dataset('mpg').dropna()\n",
    "vehicle_data = vehicle_data.sort_values('horsepower', ascending=True)\n",
    "vehicle_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 392 datapoints and 8 potential features (plus our observations, `mpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to fit a line to the below plot, which shows `mpg` vs. `horsepower` for several models of automobiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Ordinary Least Squares\n",
    "\n",
    "Instead of using the SLR formulation, in this lab we will practice linear algebra with Ordinary Least Squares. Recall that the Simple Linear Regression model is written as follows:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "We now use $\\theta = (\\theta_0, \\theta_1)$ so that the formulation more closely matches our multiple linear regression model:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_p x_p$$\n",
    "\n",
    "We can rewrite our multiple linear regression model using matrix notation. Let $\\mathbb{Y}$ be a vector of all $n$ observations in our sample. Then our prediction vector $\\hat{\\mathbb{Y}}$ is\n",
    "\n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "where $\\mathbb{X}$ is the **design matrix** representing the $p$ features for all $n$ datapoints in our sample.\n",
    "\n",
    "Note that for our SLR model, $p = 1$ and therefore the matrix notation seems rather silly. Nevertheless it is valuable to start small and build on our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 1a: Construct $\\mathbb{X}$ with an intercept term\n",
    "\n",
    "Because we have an intercept term $\\theta_0$ in our parameter vector $\\theta$, our design matrix $\\mathbb{X}$ for $p$ features actually has dimension\n",
    "\n",
    "$$ \\Large \\mathbb{X} \\in \\mathbb{R}^{n \\times (p + 1)}$$\n",
    "\n",
    "Therefore, the resulting matrix expression $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$ represents $n$ linear equations, where equation $i$ is $\\hat{y_i} = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x_1 + \\dots + \\theta_p x_p$. The constant all-ones column of $\\mathbb{X}$ is sometimes called the bias feature; $\\theta_0$ is frequently called the **bias or intercept term**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, implement `add_intercept`, which computes a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column `bias_feature` using the `np.ones` function (NumPy [documentation](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones)). This is then piped into a call to `np.concatenate` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)), which we've implemented for you.\n",
    "\n",
    "Note: `bias_feature` should be a matrix of dimension `(n,1)`, not a vector of dimension `(n,)`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"\n",
    "    Return X with a bias feature.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D dataframe of p numeric features\n",
    "    (may also be a 2D numpy array) of shape n x p\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 2D matrix of shape n x (p + 1), where the leftmost\n",
    "    column is a column vector of 1's\n",
    "    \"\"\"\n",
    "    bias_feature = ...\n",
    "    return np.concatenate([bias_feature, X], axis=1)\n",
    "\n",
    "# Note the [[ ]] brackets below: the argument needs to be\n",
    "# a matrix (DataFrame), as opposed to a single array (Series).\n",
    "X = add_intercept(vehicle_data[['horsepower']])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 1b: Define the OLS Model\n",
    "\n",
    "The predictions for all $n$ points in our data are (note $\\theta = (\\theta_0, \\theta_1, \\dots, \\theta_p)$) :\n",
    "$$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$\n",
    "\n",
    "Below, implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [pd.DataFrame.dot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `numpy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined above.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta1, theta2, ...])\n",
    "    X: a 2D dataframe of numeric features (may also be a 2D numpy array)\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined above.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Question 1c: Least Squares Estimate, Analytically\n",
    "\n",
    "Recall from lecture that Ordinary Least Squares is when we fit a linear model with mean squared error, which is equivalent to the following optimization problem:\n",
    "\n",
    "$$\\Large \\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{Y}||^2$$\n",
    "\n",
    "We showed in Lecture that the optimal estimate $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation:\n",
    "\n",
    "$$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$\n",
    "\n",
    "Below, implement the analytic solution to $\\hat{\\theta}$ using `np.linalg.inv` ([link](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$.\n",
    "\n",
    "Reminder: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()` ([link](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T)).\n",
    "\n",
    "Note: You can also consider using `np.linalg.solve` ([link](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)) instead of `np.linalg.inv` because it is more robust (more on StackOverflow [here](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li)). \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our\n",
    "    least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D dataframe (or numpy array) of numeric features\n",
    "    y: a 1D vector of tip amounts\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta (a 1D vector) computed using the\n",
    "    equation mentioned above.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "Y = vehicle_data['mpg']\n",
    "analytical_thetas = get_analytical_sol(X, Y)\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now, let's analyze our model's performance. Your task will be to interpret the model's performance using the two visualizations and one performance metric we've implemented below.\n",
    "\n",
    "First, we run **`sns.lmplot`**, which will both provide a scatterplot of `mpg` vs `horsepower` and display the least-squares line of best fit. (If you'd like to verify the OLS fit you found above is the same line found through Seaborn, change `include_OLS` to `True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_OLS = False # change this flag to visualize OLS fit\n",
    "\n",
    "sns.lmplot(x='horsepower', y='mpg', data=vehicle_data);\n",
    "predicted_mpg_hp_only = linear_model(analytical_thetas, X)\n",
    "if include_OLS:\n",
    "    # if flag is on, add OLS fit as a dotted red line\n",
    "    plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **plot the residuals.** While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only)\n",
    "plt.axhline(0, c='black', linewidth=1)\n",
    "plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$')\n",
    "plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we compute the **Multiple $R^2$** metric. As described in Lecture 11 ([link](https://docs.google.com/presentation/d/15eEbroVt2r36TXh28C2wm6wgUHlCBCsODR09kLHhDJ8/edit#slide=id.g1163459c7f0_0_86)),\n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$\n",
    "\n",
    "$R^2$  can be used\n",
    "in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature.  In SLR, $r^{2}$ and Multiple $R^{2}$ are\n",
    "equivalent; the proof is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y)\n",
    "\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 1d\n",
    "\n",
    "In the cell below, comment on the above visualization and performance metrics, and whether `horsepower` and `mpg` have a good linear fit.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Transform a Single Feature\n",
    "\n",
    "The Tukey-Mosteller Bulge Diagram tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit.\n",
    "\n",
    "Let's consider the following linear model:\n",
    "\n",
    "$$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---\n",
    "### Question 2a\n",
    "\n",
    "In the cell below, explain why we use the term \"linear\" to describe the model above, even though it incorporates a square-root of horsepower  as a feature.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to `sklearn`\n",
    "\n",
    "Yet another way to fit a linear regression model is to use **scikit learn**, an industry standard package for machine learning applications. Because it is application-specific, `sklearn` is often faster and more robust than the analytical/`scipy`-based computation methods we've used thus far.\n",
    "\n",
    "To use `sklearn`:\n",
    "\n",
    "1. Create an `sklearn` object\n",
    "1. `fit` the object to data\n",
    "1. Analyze fit or call `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create object.** We first create a `LinearRegression` object. Here's the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Note that by default, the object will include an intercept term when fitting.\n",
    "\n",
    "Here, `model` is like a \"blank slate\" for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. just run this cell\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. `fit` the object to data.** Now, we need to tell `model` to \"fit\" itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk).\n",
    "\n",
    "_**Note**: `X` needs to be a matrix (or DataFrame), as opposed to a single array (or Series). This is because `sklearn.linear_model` is robust enough to be used for multiple regression, which we will look at later this lab._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. run this cell to add sqrt(hp) column for each car in the dataset\n",
    "vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower'])\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. run this cell\n",
    "model.fit(X = vehicle_data[['sqrt(hp)']], y= vehicle_data['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Analyze fit.** Now that the model exists, we can look at the $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$ values it found, which are given in the attributes `intercept` and `coef`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 (continued). Call `predict`.** To use the `scikit-learn` linear regression model to make predictions, you can use the `model.predict` method.\n",
    "\n",
    "Below, we find the estimated `mpg` for a single datapoint with a `sqrt(hp)` of 6.78 (i.e., horsepower 46).\n",
    "\n",
    "Note that unlike the linear algebra approach, we do not need to manually add an intercept term, because our `model` (which was created with `fit_intercept=True`) will auto-add one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_datapoint = [[6.78]] # needs to be a 2D array since the X in step 2 was a 2D array.\n",
    "model.predict(single_datapoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2b\n",
    "\n",
    "Using the model defined above, set `predicted_mpg` to the predicted `mpg` for the data below. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mpg_hp_sqrt = ...\n",
    "\n",
    "# do not modify below this line\n",
    "r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "\n",
    "sns.lmplot(x='horsepower', y='mpg', data=vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt,\n",
    "         color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows a slight improvement, but note that the underlying pattern is parabolic--suggesting that perhaps we should try a quadratic feature. Next, we use the power of multiple linear regression to **add an additional feature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Add an Additional Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of this lab, we move from SLR to multiple linear regression.\n",
    "\n",
    "Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems you will often want to use **multiple features** to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data.\n",
    "\n",
    "We can consider including functions of existing features as **new features** to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.)\n",
    "\n",
    "The cell below adds a column which contains the square of the horsepower for each car in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---\n",
    "## Question 3\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Using scikit learn's `LinearRegression`, create and fit a model that tries to predict `mpg` from `horsepower` AND `hp^2` using the DataFrame `vehicle_data`. Name your model `model_multi`.\n",
    "\n",
    "**Hint**: We did something very similar in Question 2.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi = LinearRegression() # by default, fit_intercept=True\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can see the coefficients and intercept. Note, there are now two elements in `model_multi.coef_`, since there are two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Using the above values, in LaTeX, write out the function that the model is using to predict `mpg` from `horsepower` and `hp^2`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "The plot below shows the prediction of our model. It's much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']])\n",
    "r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi)\n",
    "\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_only, label='hp only');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "In the cell below, we assign the mean of the `mpg` column of the vehicle `data` dataframe to `mean_mpg`. Given this information, what is the mean of the `mean_predicted_mpg_hp_only`, `predicted_mpg_hp_sqrt`, and `predicted_mpg_multi` arrays?\n",
    "\n",
    "Hint: You should not have to call `np.mean` in your code.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mpg = np.mean(vehicle_data['mpg'])\n",
    "mean_predicted_mpg_hp_only = ...\n",
    "mean_predicted_mpg_hp_sqrt = ...\n",
    "mean_predicted_mpg_multi = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Faulty Feature Engineering: Redundant Features\n",
    "\n",
    "Suppose we used the following linear model:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{mpg} &= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\\\\n",
    "&\\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower}\n",
    "\\end{align}\n",
    "\n",
    "Notice that `horsepower` appears twice in our model!! We will explore how this redundant feature affects our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "### Question 4a: Linear Algebra\n",
    "\n",
    "Construct a matrix `X_redundant` that uses the vehicle `data` DataFrame to encode the \"three\" features above, as well as a bias feature.\n",
    "\n",
    "**Hint**: Use the `add_intercept` term you implemented in Question 1a.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_redundant = ...\n",
    "X_redundant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Now, run the cell below to find the analytical OLS Estimate using the `get_analytical_sol` function you wrote in Question 1c.\n",
    "\n",
    "Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# the try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg'])\n",
    "    analytical_thetas\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict `mpg`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Note: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums (to be covered in Gradient Descent lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# sklearn finds optimal parameters despite redundant features\n",
    "model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column\n",
    "model_redundant.fit(X = X_redundant, y = vehicle_data['mpg'])\n",
    "model_redundant.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Overfitting with Too Many Features\n",
    "\n",
    "Let's take what we've learned so far and go one step further: introduce even more features.\n",
    "\n",
    "Again, using scikit learn's `LinearRegression`, we fit a model that tries to predict `mpg` using each of the following as features:\n",
    "- `horsepower`\n",
    "- `hp^2`\n",
    "- `model_year`\n",
    "- `acceleration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "desired_columns = ['horsepower', 'hp^2', 'model_year', 'acceleration']\n",
    "model_overfit = LinearRegression()\n",
    "model_overfit.fit(X = vehicle_data[desired_columns], y= vehicle_data['mpg'])\n",
    "predicted_mpg_overfit = model_overfit.predict(vehicle_data[['horsepower', 'hp^2', 'model_year', 'acceleration']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "The plot below shows the prediction of our more sophisticated model. Note we arbitrarily plot against horsepower for the ease of keeping our plots 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_overfit, color = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what you see in the above plot. Why is the shape of our prediction curve so jagged? Do you think this is a good model to predict the `mpg` of some car we don't already have information on?\n",
    "\n",
    "This idea –the **bias-variance tradeoff**– is an idea we will explore in the coming weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Question 5: Comparing $R^2$\n",
    "\n",
    "Lastly, set `r2_overfit` to be the multiple $R^2$ coefficient obtained by using `model_overfit`.\n",
    "\n",
    "- Hint: This is very similar to several pre-computed cells in Questions 1c, 2b, and 3b.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_overfit = ...\n",
    "r2_overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this model with previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# compares q1, q2, q3, and overfit models (ignores redundant model)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "print('Multiple R^2 using both hp and hp^2: ', r2_multi)\n",
    "print('Multiple R^2 using hp, hp^2, model year, and acceleration: ', r2_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was done correctly, the multiple $R^2$ of our latest model should be substantially higher than that of the previous models. This is because multiple $R^2$ increases with the number of covariates (i.e., features) we add to our model. \n",
    "\n",
    "<br/>\n",
    "\n",
    "**A Word on Overfitting**: We might not always want to use models with large multiple $R^2$ values because these models could be **overfitting** to our specific sample data, and won't generalize well to unseen data from the population. Again, this is an idea we will explore in future lectures and assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You finished the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
