{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 6: Modeling, Summary Statistics and Loss Functions\n",
    "\n",
    "This lab will utilize the \"flipped classroom\" model, where you will be introduced and exposed to some concepts in the lab, and then will learn more about the theory in lecture.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "1. Define loss functions and find the arguments that minimize them.\n",
    "2. Explore different statistics and techniques for optimization.\n",
    "\n",
    "**This assignment should be completed and submitted by Tuesday, October 5th, 2021 at 11:59 PM PDT**.\n",
    "\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*List collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Constant Model and Loss Functions\n",
    "\n",
    "### Constant Model\n",
    "\n",
    "In the modeling context, $y$ represents our \"true observations\", which are typically what we are trying to model. $\\hat{y}$ represents our prediction (for any model). In this lab, we will use the constant model, where our prediction for any input is a constant:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta\n",
    "$$\n",
    "\n",
    "$\\theta$ is what we call a **parameter**. Our goal is to find the value of our parameters that **best fit our data**. We represent the optimal parameter(s) with $\\hat{\\theta}$.\n",
    "\n",
    "We call the constant model a **summary statistic**, as we are determining one number that best \"summarizes\" a set of values.\n",
    "\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Loss functions are what we use to determine the optimal parameter(s) for our model.\n",
    "\n",
    "A loss function is a measure of how well a model is able to predict the expected outcome. In other words, it measures the deviations of the predicted values from the observed values. In this lab we will implement the squared loss and absolute loss functions.  \n",
    "\n",
    "In the formulations below $y$ represents the observed values and $\\hat{y}$ stands for our prediction.\n",
    "\n",
    "1. **Squared Loss** (also known as the $L_2$ loss, pronounced \"ell-two\"):\n",
    "\n",
    "$$\\Large L(y, \\hat{y}) = (y - \\hat{y})^2$$\n",
    "\n",
    "2. **Absolute Loss** (also known as the $L_1$ loss, pronounced \"ell-one\"):\n",
    "\n",
    "$$\\Large L\\left(y, \\hat{y} \\right) = \\left| y - \\hat{y} \\right|$$\n",
    "\n",
    "Since we are using the constant model $\\hat{y} = \\theta$ for the remainder of the lab, we will instead refer to these loss functions as being $(y - \\theta)^2$ and $|y - \\theta|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "33c63379-d85b-4638-8183-d008fdb96de7",
    "_uuid": "7ad7f9f24df7dba8ac92d234890835f6b9970834",
    "nbgrader": {
     "grade": false,
     "grade_id": "imports1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Implement the Squared Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Question 1a\n",
    "\n",
    "$$\\Large\n",
    "L\\left(y,  \\theta \\right) = \\left( y - \\theta \\right)^2\n",
    "$$\n",
    "\n",
    "Based on the comments below, implement the squared loss function. Your answer should not use any loops.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_obs, theta):\n",
    "    \"\"\"\n",
    "    Calculate the squared loss of the observed data and a summary statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    y_obs: an observed value\n",
    "    theta : some constant representing a summary statistic\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The squared loss between the observation and the summary statistic.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1b: Plotting the Squared Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let us now consider the case where `y_obs` equals 10. For arbitrary values of `theta`, plot the squared loss using the function above\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = 10\n",
    "theta_values = np.linspace(0, 20, 100) # some arbitrary values of theta\n",
    "...\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L2 loss')\n",
    "plt.title(r'L2 Loss for different values of $\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Mean Squared Error for the Tips Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our knowledge to some real world data. In this section, you will try to find the best statistic $\\theta$ to represent the tips given in an array. The simple procedure you will use in this lab includes constructing the mean squared error (MSE) for the tips data and finding the value that minimizes the MSE. Below you are given an array of tips from a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loaddata",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell, do not change anything\n",
    "df = sns.load_dataset(\"tips\")\n",
    "tips = np.array(df['tip']) # array of observed tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, we can extend the above loss functions to an entire dataset by taking the average. Let the dataset $\\mathcal{D}$ be the set of observations:\n",
    "\n",
    "$$\\Large\\mathcal{D} = \\{y_1, \\ldots, y_n\\}$$\n",
    "\n",
    "where $y_i$ is the $i^{th}$ tip.\n",
    "\n",
    "We can define the average loss over the dataset as:\n",
    "\n",
    "$$\\Large\n",
    "R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\theta)\n",
    "$$\n",
    "\n",
    "Define the `mean_squared_error` function which computes the mean squared error given the data and a value for `theta`. Assume that `data` will be a numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(theta, data):\n",
    "    ...\n",
    "\n",
    "mean_squared_error(5.3, tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below  we will plot the mean squared error for different theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_values = np.linspace(0, 6, 100)\n",
    "mse = [mean_squared_error(theta, tips) for theta in theta_values]\n",
    "plt.plot(theta_values, mse)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L2 loss')\n",
    "plt.title(r'L2 Loss for different values of $\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Find the value of `theta` that minimizes the L2 loss above via observation of the plot above. Round your answer to the nearest integer.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_observed_mse = ...\n",
    "min_observed_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Find the Minimizing Value for Our Tips Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots some arbitrary 4th degree polynomial function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-4, 2.5, 100)\n",
    "\n",
    "def fx(x):\n",
    "    return 0.1 * x**4 + 0.2*x**3 + 0.2 * x **2 + 1 * x + 10\n",
    "\n",
    "plt.plot(x_values, fx(x_values));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the plot, we see that the x which minimizes the function is slightly larger than -2. What if we want the exact value? We will demonstrate grabbing the minimum value and the optimal `x` in the following cell.\n",
    "\n",
    "The function `minimize` from [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) will attempt to minimize any function you throw at it.\n",
    "\n",
    "Try running the cell below, and you will see that `minimize` seems to get the answer correct.\n",
    "\n",
    "Note: For today, we'll let minimize work as if by magic. We'll discuss how `minimize` works later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "minimize(fx, x0 = 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fun` value is the minimum value of the function. The `x` is the x which minimizes the function. We can index into the object returned by `minimize` to get these values. We have to add the additional `[0]` at the end because the minimizing x is returned as an array, but this is not necessarily the case for other attributes (i.e. `fun`). The reason for this is that `minimize` can also minimize multivariable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimization_result_for_fx = minimize(fx, x0 = 0)\n",
    "min_of_fx = minimization_result_for_fx['fun']\n",
    "x_which_minimizes_fx = minimization_result_for_fx['x'][0]\n",
    "min_of_fx, x_which_minimizes_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `x0` that we passed to the `minimize` function is where the `minimize` function starts looking as it tries to find the minimum. For example, above, `minimize` started its search at x = 1.1 because that's where we told it to start. For the function above, it doesn't really matter what x we start at because the function is nice and has only a single local minimum. More technically, the function is nice because it is [convex](https://en.wikipedia.org/wiki/Convex_function), a property of functions that we will discuss later in the course.\n",
    "\n",
    "`minimize` isn't perfect. For example, if we give it a function with many valleys (also known as local minima) it can get stuck. For example, consider the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_values = np.linspace(-2, 10, 100)\n",
    "\n",
    "def fw(w):\n",
    "    return 0.1 * w**4 - 1.5*w**3 + 6 * w **2 - 1 * w + 10\n",
    "\n",
    "plt.plot(w_values, fw(w_values));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start the minimization at w = 6.5, we'll get stuck in the local minimum at w = 7.03. Note that no matter what your actual variable is called in your function, the `minimize` routine still calls the starting point `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(fw, x0 = 6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using the `minimize` function, find the value of `theta` that minimizes the mean squared error for our tips dataset. In other words, you want to find the exact minimum of the plot that you saw in question 2.\n",
    "\n",
    "For autograding purposes, assign `min_scipy` to the value of `theta` that minimizes the MSE according to the `minimize` function.\n",
    "\n",
    "Hint: You can't pass your `mean_squared_error` function to `minimize` because `mean_squared_error` has two variables: `theta` and `data`. `minimize` will get confused because it thinks it needs to minimize by picking the best `theta` and best `data` values. We only want it to play around with `theta`.\n",
    "\n",
    "In other words, you need to pass a function of one variable `theta` to the `minimize` function, which means you'll need to create a new function of only ONE variable `theta`. This is very simple, but also very tricky when you do this for the first time. Make sure to ask for help if you get stuck.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_scipy = minimize(..., x0=0.0)['x'][0] \n",
    "min_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In lecture, we will show that the value of `theta` that minimizes the mean squared error is the average of the data for the constant model. Assign `min_computed` to the mean of the tips dataset, and compare this to the values you observed in questions 2b and 3a.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_computed = ...\n",
    "min_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflecting on the lab so far, we've now seen 3 ways to find the summary statistic `theta` that minimizes the mean squared error:\n",
    "1. Create a plot of the MSE for the given data array vs. `theta` and eyeball the minimizing `theta`.\n",
    "2. Create a function that returns the MSE for a specific data array as a function of `theta` and use the scipy `minimize` function to find the exact `theta` which minimizes this function.\n",
    "3. Simply compute the `mean` of the data array.\n",
    "\n",
    "At this point, you've hopefully convinced yourself that the `mean` of the data is the summary statistic that minimizes mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4: Implement the Absolute Loss \n",
    "\n",
    "\n",
    "In this section, you will follow the exact same steps as above but for the absolute loss function. Absolute loss is defined as:\n",
    "\n",
    "$$\\Large\n",
    "L\\left(y, \\theta \\right) = \\left| y - \\theta \\right|\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "-->\n",
    "In the cell below define the function `abs_loss` which returns the absolute loss given a value of `theta` and `y_obs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(theta, y_obs):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will plot the absolute loss for different values of `theta` given that `y_obs = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = 10\n",
    "theta_values = np.linspace(0, 20, 100) # some arbitrary values of theta\n",
    "plt.plot(theta_values, abs_loss(theta_values, y_obs))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L1 loss')\n",
    "plt.title('L1 Loss for different values of theta');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Question**: How are outliers penalized differently in absolute loss compared to square loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Mean Absolute Error for the Tips Data\n",
    "\n",
    "### Question 5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Define the `mean_absolute_error` function which computes the mean absolute error (MAE) given the data and a value for `theta`. Assume that `data` will be a numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(theta, data):\n",
    "    ...\n",
    "\n",
    "mean_absolute_error(5.3, tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the mean absolute error for different theta values on the tips dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_values = np.linspace(0, 6, 100)\n",
    "mae = [mean_absolute_error(theta, tips) for theta in theta_values]\n",
    "plt.plot(theta_values, mae)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L1 loss')\n",
    "plt.title(r'L1 Loss for different values of $\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the plot looks somewhat similar the plot of the mean squared error. Try to identify any key differences you observe and write them down below. This might be more fun with a partner. Note, your answer will not be graded, so don't worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To minimize the function, let's zoom in closer to the minimizing `theta`. We will plot the mean absolute error again using the given `theta_values` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_values = np.linspace(2.7, 3.02, 100)\n",
    "mae = [mean_absolute_error(theta, tips) for theta in theta_values]\n",
    "plt.plot(theta_values, mae)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L1 Loss')\n",
    "plt.title(r'L1 Loss for different values of $\\theta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5b\n",
    "\n",
    "This time, observe that the function is piecewise linear and has a slope of zero near its minimum. Because of the large flat region at the minimum, there are multiple values of `theta` that minimize the L1 loss.\n",
    "\n",
    "Give a `theta` rounded to the nearest tenth that minimizes L1 loss. By \"rounded to the nearest tenth\" we mean you'd say 7.6 instead of 7.55.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_observed_mae = ...\n",
    "min_observed_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Find the Minimizing Value Using Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6a\n",
    "\n",
    "As before, we will use the `minimize` function to find a solution. Note that we will be minimizing the mean absolute error for our tips dataset. Assign `min_abs_scipy` to the value of `theta` that minimizes the MAE according to the `minimize` function for the `tips` data. Note: Depending on the `x0` value you specify, you will get different results! \n",
    "\n",
    "Extra: Try various `x0` values and record the different outputs you get from `minimize`. Use the plot you created above to verify that these are all valid minimizing statistics.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_abs_scipy = minimize(..., x0=0.0)['x'][0]\n",
    "min_abs_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the MSE, there are three ways to compute the summary statistic `theta` that minimizes the MAE:\n",
    "1. Create a plot of the MAE for the given data array vs. `theta` and eyeball a minimizing `theta`.\n",
    "2. Create a function that returns the MAE for a specific data array as a function of `theta` and use the scipy `minimize` function to find an exact `theta` which minimizes this function.\n",
    "3. Simply compute the ?????? of the data array.\n",
    "\n",
    "Try to figure out what to substitute in for the ?????? above. To this, try out various statistics functions provided by `np`. A list and documentation is available at [https://docs.scipy.org/doc/numpy/reference/routines.statistics.html](https://docs.scipy.org/doc/numpy/reference/routines.statistics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6b\n",
    "\n",
    "Assign `min_abs_computed` to the correct summary statistic using method `#3` outlined in the the previous problem.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_abs_computed = ...\n",
    "min_abs_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Observations on Differences Between MAE vs. MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in this lab, we said we'd describe our observations about the differences between the MAE and MSE.\n",
    "\n",
    "There are three key differences that we identified between the plots of the MSE and MAE.\n",
    "\n",
    "1. The minimizing $\\theta$ is different.\n",
    "2. The plot for MAE increases linearly instead of quadratically as we move far away from the minimizing $\\theta$.\n",
    "3. The plot for MAE is piecewise linear instead of smooth. Each change in slope happens at the same $\\theta$ value as a data point in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You finished the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "301px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
