{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab07.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 7: Gradient Descent and Feature Engineering\n",
    "\n",
    "In this lab, we will work through the process of:\n",
    "1. Defining loss functions\n",
    "1. Feature engineering\n",
    "1. Minimizing loss functions using numeric methods and analytical methods \n",
    "1. Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features\n",
    "1. Computing a gradient for a nonlinear model\n",
    "1. Using gradient descent to optimize the nonline model\n",
    "\n",
    "This lab will continue using the toy `tips` calculation dataset used in Labs 5 and 6.\n",
    "\n",
    "**This assignment should be completed and submitted by Tuesday, March 8th, 2022 at 11:59pm PDT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"LQN3dv-WozY\", list = 'PLQCcNQgUcDfq7hfzndTXvUrFN_MoUN2F1', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *List names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Loading the Tips Dataset\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, we'll be trying to predict tips from the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intro to Feature Engineering\n",
    "\n",
    "So far, we've only considered models of the form $\\hat{y} = f_{\\theta}(x) = \\sum_{j=0}^d x_j\\theta_j$, where $\\hat{y}$ is quantitative continuous. \n",
    "\n",
    "We call this a linear model because it is a linear combination of the features (the $x_j$). However, our features don't need to be numbers: we could have categorical values such as names. Additionally, the true relationship doesn't have to be linear, as we could have a relationship that is quadratic, such as the relationship between the height of a projectile and time.\n",
    "\n",
    "In these cases, we often apply **feature functions**, functions that take in some value and output another value. This might look like converting a string into a number, combining multiple numeric values, or creating a boolean value from some filter.\n",
    "\n",
    "Then, if we call $\\phi$ (\"phi\") our \"phi\"-ture function, our model takes the form $\\hat{y} = f_{\\theta}(x) = \\sum_{j=0}^d \\phi(x)_j\\theta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example feature functions\n",
    "\n",
    "1. One-hot encoding\n",
    "    - converts a single categorical feature into many binary features, each of which represents one of the possible values in the original column\n",
    "    - each of the binary feature columns produced contains a 1 for rows that had that column's label in the original column, and 0 elsewhere\n",
    "1. Polynomial features\n",
    "    - create polynomial combinations of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Defining the Model and Feature Engineering\n",
    "\n",
    "In Lab 6 we used the constant model. Now let's make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below:\n",
    "\n",
    "$$ \\text{Tip} = \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size} $$\n",
    "\n",
    "Unfortunately, that's not possible because some of these features like \"day\" are not numbers, so it doesn't make sense to multiply by a numerical parameter.\n",
    "\n",
    "Let's start by converting some of these non-numerical values into numerical values. Before we do this, let's separate out the tips and the features into two separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### Question 1a: Feature Engineering\n",
    "\n",
    "First, let's convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. \n",
    "\n",
    "For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can lower the accuracy of our model.\n",
    "\n",
    "Instead, let's use one-hot encoding to better represent these features! \n",
    "\n",
    "As you will learn in lecture, one-hot encoding is a way that we can produce a binary vector to indicate non-numeric features. \n",
    "\n",
    "In the `tips` dataset for example, we encode Sunday as the vector `[0 0 0 1]` because our dataset only contains bills from Thursday through Sunday. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset. This dataframe holds our \"featurized\" data, which is also often denoted by $\\phi$.\n",
    "\n",
    "**Hint:** You may find the [pd.get_dummies method](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) or the [DictVectorizer class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) useful when doing your one-hot encoding.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### Question 1b: Defining the Model\n",
    "\n",
    "Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 12 features instead of 6. Therefore, our linear model now looks like:\n",
    "\n",
    "$$ \\text{Tip} = \\theta_1 \\cdot \\text{size} + \\theta_2 \\cdot \\text{total}\\_\\text{bill} + \\theta_3 \\cdot \\text{day}\\_\\text{Thur} + \\theta_4 \\cdot \\text{day}\\_\\text{Fri} + ... + \\theta_{11} \\cdot \\text{time}\\_\\text{Lunch} + \\theta_{12} \\cdot \\text{time}\\_\\text{Dinner} $$\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. Implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "Below, we create a `MyLinearModel` class with two methods, `predict` and `fit`. When fitted, this model fails to do anything useful, setting all of its 12 parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        number_of_features = X.shape[1]\n",
    "        self._thetas = np.zeros(shape = (number_of_features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLinearModel()\n",
    "model.fit(one_hot_X, tips)\n",
    "model._thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Fitting a Linear Model using scipy.optimize.minimize Methods\n",
    "\n",
    "Recall in Lab 5 and in lecture 12 we defined multiple loss functions and found the optimal theta using the `scipy.optimize.minimize` function. Adapt the code below to implement the fit method of the linear model.\n",
    "\n",
    "Note that we've added a `loss_function` parameter where the model is fit using the desired loss function, i.e. not necssarily the L2 loss. Example loss function are given as `l1` and `l2`.\n",
    "    \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "class MyLinearModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, loss_function, X, y):\n",
    "        \"\"\"\n",
    "        Produce the estimated optimal _thetas for the given loss function, \n",
    "        feature matrix X, and observations y.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        loss_function: either the squared or absolute loss functions defined above\n",
    "        X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded)\n",
    "        y: a 1D vector of tip amounts\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        The estimate for the optimal theta vector that minimizes our loss\n",
    "        \"\"\"\n",
    "        \n",
    "        number_of_features = X.shape[1]\n",
    "\n",
    "        ## Notes on the following function call which you need to finish:\n",
    "        # \n",
    "        # 0. The starting guess should be some arbitrary array of the correct length.\n",
    "        #    Note the \"number of features\" variable above.\"\n",
    "        # 1. The ... in \"lambda theta: ...\" should be replaced by the average loss if we\n",
    "        #    compute X @ theta. The loss is measured using the given loss function,\n",
    "        #    relative to the observations in the variable y.\n",
    "        \n",
    "        starting_guess = ...\n",
    "        self._thetas = minimize(lambda theta: \n",
    "                                ...\n",
    "                                , x0 = starting_guess)['x']\n",
    "        # Notice above that we extract the 'x' entry in the dictionary returned by `minimize`. \n",
    "        # This entry corresponds to the optimal theta estimated by the function. Sorry\n",
    "        # we know it's a little confusing, but 'x' is hard coded into the minimize function\n",
    "        # because of the fact that in the optimization universe \"x\" is what you optimize over.\n",
    "        # It'd be less confusing for DS100 students if they used \"theta\".\n",
    "        \n",
    "# When you run the code below, you should get back some non zero thetas.\n",
    "        \n",
    "model = MyLinearModel()\n",
    "model.fit(l2, one_hot_X, tips)\n",
    "model._thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for your model above should be just slightly larger than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(model.predict(one_hot_X), tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Fitting the Model using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{y}||^2$$\n",
    "\n",
    "We showed in [Lecture 11](https://docs.google.com/presentation/d/15eEbroVt2r36TXh28C2wm6wgUHlCBCsODR09kLHhDJ8/edit#slide=id.g113dfce000f_0_2682) that the optimal $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation: $(X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### Question 3a: Analytic Solution Using Explicit Inverses\n",
    "\n",
    "For this problem, implement the analytic solution above using `np.linalg.inv` to compute the inverse of $X^TX$.\n",
    "\n",
    "Reminder: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAnalyticallyFitOLSModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Sets _thetas using the analytical solution to the ordinary least squares problem\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded)\n",
    "        y: a 1D vector of tip amounts\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        The estimate for theta computed using the equation mentioned above\n",
    "        \"\"\"\n",
    "        \n",
    "        xTx = ...\n",
    "        xTy = ...\n",
    "        self._thetas = ...\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to find the analytical solution for the `tips` dataset. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you run the code below, you should get back some non zero thetas.\n",
    "        \n",
    "model = MyAnalyticallyFitOLSModel()\n",
    "model.fit(one_hot_X, tips)\n",
    "analytical_thetas = model._thetas\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution for our one-hot encoded `tips` dataset.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 3b: Fixing our One-Hot Encoding\n",
    "\n",
    "Now, let's fix our one-hot encoding approach from question 1 so we don't get the error we saw in question 3a. Complete the code below to one-hot-encode our dataset such that `one_hot_X_revised` has no redundant features.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_revised(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features without any redundancies.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "\n",
    "one_hot_X_revised = one_hot_encode_revised(X)    \n",
    "    \n",
    "numerical_model = MyLinearModel()\n",
    "numerical_model.fit(l2, one_hot_X_revised, tips)\n",
    "    \n",
    "analytical_model = MyAnalyticallyFitOLSModel()\n",
    "analytical_model.fit(one_hot_X_revised, tips)\n",
    "\n",
    "\n",
    "print(\"Our numerical model's loss is: \", mean_squared_error(numerical_model.predict(one_hot_X_revised), tips))\n",
    "print(\"Our analytical model's loss is: \", mean_squared_error(analytical_model.predict(one_hot_X_revised), tips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 3c: Analyzing our new One-Hot Encoding\n",
    "\n",
    "Why did removing redundancies in our one-hot encoding fix the problem we had in 3a?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Note: An alternate approach is to use `np.linalg.solve` instead of `np.linalg.inv`. For the example above, even with the redundant features, `np.linalg.solve` will work well. Though in general, it's best to drop redundant features anyway.\n",
    "\n",
    "In case you want to learn more, here is a relevant Stack Overflow post: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data for this problem\n",
    "df = pd.read_csv(\"lab7_data.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot this data, we see that there is a clear sinusoidal relationship between x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.scatter(df, x = \"x\", y = \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll show gradient descent is so powerful it can even optimize a nonlinear model. Specifically, we're going to model the relationship of our data by:\n",
    "\n",
    "$$\\Large{\n",
    "f_{\\boldsymbol{\\theta(x)}} = \\theta_1x + sin(\\theta_2x)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. \n",
    "\n",
    "Here, we're assuming the amplitude $a$ is around 1, and the phase shifting parameter $c$ is around zero. We do not attempt to justify this assumption and you're welcome to see what happens if you ignore this assumption at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask why we don't just create a linear model like we did earlier with a sinusoidal feature. The issue is that the theta is INSIDE the sin function. In other words, linear models use their parameters to adjust the scale of each feature, but $\\theta_2$ in this model adjusts the frequency of the feature. There are tricks we could play to use our linear model framework here, but we won't attempt this in our lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Recall $\\hat{\\theta}$ is the value of $\\theta$ that minimizes our loss function. One way of solving for $\\hat{\\theta}$ is by computing the gradient of our loss function with respect to $\\theta$, like we did in lecture: https://docs.google.com/presentation/d/1j9ESgjn-aeZSOX5ON1wjkF5WBZHc4IN7XvTpYnX1pFs/edit#slide=id.gfc76b62ec3_0_27. Recall that the gradient is a column vector of two partial derivatives.\n",
    "\n",
    "Write/derive the expressions for following values and use them to fill in the functions below.\n",
    "\n",
    "* $L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$: our loss function, the mean squared error\n",
    "* $\\frac{\\partial L }{\\partial \\theta_1}$: the partial derivative of $L$ with respect to $\\theta_1$\n",
    "* $\\frac{\\partial L }{\\partial \\theta_2}$: the partial derivative of $L$ with respect to $\\theta_2$\n",
    "\n",
    "Recall that $L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (\\textbf{y}_i - \\hat{\\textbf{y}}_i)^2$\n",
    "\n",
    "Specifically, the functions `sin_MSE`, `sin_MSE_dt1` and `sin_MSE_dt2` should compute $R$, $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the expressions you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ to implement these functions. In the functions below, the parameter `theta` is a vector that looks like $\\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$. We have completed `sin_MSE_gradient`, which calls `dt1` and `dt2` and returns the gradient `dt` for you.\n",
    "\n",
    "Notes: \n",
    "* Keep in mind that we are still working with our original set of data, `df`.\n",
    "* To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array.\n",
    "* Another way to keep your code more concise is to use the function `sin_model` we defined which computes the output of the model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_MSE(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the l2 loss of our sinusoidal model given theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def sin_MSE_dt1(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def sin_MSE_dt2(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def sin_MSE_gradient(theta, x, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4b: Implementing Gradient Descent and Using It to Optimize the Sin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's now implement gradient descent. \n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below) than the `gradient_descent` [implementation from lecture](https://ds100.org/sp22/resources/assets/lectures/lec12/lec12.html).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.array([0, 0])\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    data -- the data used in the model \n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, df, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass the tests above, you're done coding for this lab, though there are some cool visualizations below we'd like you to think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. The code below plots our $x$-values with our model's predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = init_theta()\n",
    "\n",
    "theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our model output over our observaitons shows that gradient descent did  a great job finding both the overall increase (slope) of the data, as well as the oscillation frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df['x'], df['y']\n",
    "y_pred = sin_model(x, theta_est)\n",
    "\n",
    "plt.plot(x, y_pred, label='Model ($\\hat{y}$)')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation ($y$)', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Visualizing Loss (Extra)\n",
    "\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.\n",
    "\n",
    "In the previous plot we saw the loss decrease with each iteration. In this part, we'll see the trajectory of the algorithm as it travels the loss surface? Run the following cells to see visualization of this trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.array(thetas).squeeze()\n",
    "loss = np.array(loss)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "from lab7_utils import plot_3d\n",
    "plot_3d(thetas[:, 0], thetas[:, 1], loss, mean_squared_error, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(theta1_s, theta2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for theta1, theta2 in data:\n",
    "        l = loss_function(model(x, np.array([theta1, theta2])), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    # Create the contour \n",
    "    theta_points = go.Scatter(name=\"theta Values\", \n",
    "                              x=theta_1_series, \n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "    lr_loss_contours = go.Contour(x=theta1_s, \n",
    "                                  y=theta2_s, \n",
    "                                  z=z, \n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Static Learning Rate', thetas, mean_squared_error, sin_model, df[\"x\"], df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, gradient descent is able to navigate even this fairly complex loss space and find a nice minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You finished the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
